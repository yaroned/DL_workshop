{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_Language_Translator_EN-FR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "V4aFCJiq-ZH2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Upload data_utils.py\n"
      ]
    },
    {
      "metadata": {
        "id": "5ms75Nng-X49",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "cd12db5f-64a4-4b98-a6b9-197d52d7d715"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-95f41ad1-251d-4c70-807a-fa8c470f3ec2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-95f41ad1-251d-4c70-807a-fa8c470f3ec2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_util.py to data_util.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z0k-xg5W9ub8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "f10911c6-a651-4278-cfe6-e7776627e0ae"
      },
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "!mkdir Data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Running setup.py bdist_wheel for wget ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "on058dAT9yPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8e16fea4-258a-4c19-8222-d4ea82782035"
      },
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "DATADIR = \"Data\"\n",
        "filename = wget.download(\"http://opus.nlpl.eu/download.php?f=OpenSubtitles/en-fr.txt.zip\")\n",
        "with zipfile.ZipFile(filename,\"r\") as zip_ref:\n",
        "    zip_ref.extractall(DATADIR)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(os.listdir(DATADIR))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "['OpenSubtitles.en-fr.en', 'OpenSubtitles.en-fr.fr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lwR980N2EWxb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Print some of the subtitles"
      ]
    },
    {
      "metadata": {
        "id": "ZlIsJXlvEIUk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "50deffd0-8829-4ad6-bfee-5a9ad4313f33"
      },
      "cell_type": "code",
      "source": [
        "!head -10 Data/OpenSubtitles.en-fr.en"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VÉNUS lN FURS\r\n",
            "Go on!\r\n",
            "Will you sign?\r\n",
            "Go on!\r\n",
            "- Tell me first.\r\n",
            "- I don' t know.\r\n",
            "Who' s that?\r\n",
            "Howthe hell should I know?\r\n",
            "- Shall I pick it up?\r\n",
            "- No!\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JDJStFjJFC8F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "d0afa505-33c0-4a4a-9c6d-dd34a75f67f6"
      },
      "cell_type": "code",
      "source": [
        "!head -10 Data/OpenSubtitles.en-fr.fr"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VÉNUS EN FOURRURE\r\n",
            "Continue!\r\n",
            "Tu signes?\r\n",
            "Continue!\r\n",
            "- D'abord, dis- moi.\r\n",
            "- Je ne sais pas.\r\n",
            "Qui est- ce?\r\n",
            "Comment le saurais- je?\r\n",
            "- Dois- je répondre?\r\n",
            "- Non!\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xBAYygGcEKJK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3016de1-a681-47b9-943e-385864db64db"
      },
      "cell_type": "code",
      "source": [
        "!ls Data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenSubtitles.en-fr.en\tOpenSubtitles.en-fr.fr\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kVL_oUfrFNsV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare pickle"
      ]
    },
    {
      "metadata": {
        "id": "NnhXfmP19_3O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import data_util\n",
        "\n",
        "en_sentences = data_util.read_sentences('Data/OpenSubtitles.en-fr.en')\n",
        "fr_sentences = data_util.read_sentences('Data/OpenSubtitles.en-fr.fr')\n",
        "data_util.save_dataset(DATADIR+'/data.pkl', data_util.create_dataset(en_sentences, fr_sentences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iu2UCQeoFZyi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ]
    },
    {
      "metadata": {
        "id": "KfAj3DkrFbwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read dataset \n",
        "X, Y, en_word2idx, en_idx2word, en_vocab, fr_word2idx, fr_idx2word, fr_vocab = data_util.read_dataset(DATADIR+'/'+'data.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p1XyhTRMFnLp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "b81d8f14-7caa-4efe-a0a3-72baaeb19f81"
      },
      "cell_type": "code",
      "source": [
        "# inspect data\n",
        "print('Sentence in English - encoded:', X[0])\n",
        "print('Sentence in French - encoded:', Y[0])\n",
        "print('Decoded:\\n------------------------')\n",
        "\n",
        "toPrt = \"\"\n",
        "for i in range(len(X[2])):\n",
        "    toPrt += en_idx2word[X[2][i]] + \" \"\n",
        "print(toPrt)\n",
        "\n",
        "print('\\n')\n",
        "toPrt = \"\"\n",
        "\n",
        "for i in range(len(Y[2])):\n",
        "    toPrt += fr_idx2word[Y[2][i]]  + \" \"\n",
        "print(toPrt)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence in English - encoded: [0, 14665, 0]\n",
            "Sentence in French - encoded: [0, 12757, 0]\n",
            "Decoded:\n",
            "------------------------\n",
            "- I don' t know \n",
            "\n",
            "\n",
            "- Je ne sais pas \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E2IAGnGlHAwd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preprocess - Padding"
      ]
    },
    {
      "metadata": {
        "id": "WiYwZSrwHOCn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dependencies\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wPWmHvM7GqR1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data padding\n",
        "def data_padding(x, y, length = 15):\n",
        "    for i in range(len(x)):\n",
        "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
        "        y[i] = [fr_word2idx['<go>']] + y[i] + [fr_word2idx['<eos>']] + (length-len(y[i])) * [fr_word2idx['<pad>']]\n",
        "\n",
        "data_padding(X, Y)\n",
        "\n",
        "# data splitting\n",
        "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
        "\n",
        "del X\n",
        "del Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yj7ra1iJHq00",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model configurations"
      ]
    },
    {
      "metadata": {
        "id": "P3turR7eHs89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# config for the model\n",
        "\n",
        "input_seq_len = 15\n",
        "output_seq_len = 17\n",
        "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
        "fr_vocab_size = len(fr_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
        "\n",
        "# input placeholders\n",
        "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
        "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pttDz44xJyK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# target placeholders\n",
        "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
        "# add one more target\n",
        "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
        "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
        "\n",
        "# output projection\n",
        "size = 512\n",
        "w_t = tf.get_variable('proj_w', [fr_vocab_size, size], tf.float32)\n",
        "b = tf.get_variable('proj_b', [fr_vocab_size], tf.float32)\n",
        "w = tf.transpose(w_t)\n",
        "output_projection = (w, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "agAKviGHub9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Build the model"
      ]
    },
    {
      "metadata": {
        "id": "2wOjkKmKuYhi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
        "                                            encoder_inputs,\n",
        "                                            decoder_inputs,\n",
        "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
        "                                            num_encoder_symbols = en_vocab_size,\n",
        "                                            num_decoder_symbols = fr_vocab_size,\n",
        "                                            embedding_size = 100,\n",
        "                                            feed_previous = False,\n",
        "                                            output_projection = output_projection,\n",
        "                                            dtype = tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r0-2Rfk5unPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define loss Function"
      ]
    },
    {
      "metadata": {
        "id": "uE2GyGLDuplJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
        "def sampled_loss(labels, logits):\n",
        "    return tf.nn.sampled_softmax_loss(\n",
        "                        weights = w_t,\n",
        "                        biases = b,\n",
        "                        labels = tf.reshape(labels, [-1, 1]),\n",
        "                        inputs = logits,\n",
        "                        num_sampled = 512,\n",
        "                        num_classes = fr_vocab_size)\n",
        "\n",
        "# Weighted cross-entropy loss for a sequence of logits\n",
        "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xo6X8gYNuuWn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create helper functions"
      ]
    },
    {
      "metadata": {
        "id": "6B0hrzXmuxaz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# simple softmax function\n",
        "def softmax(x):\n",
        "    n = np.max(x)\n",
        "    e_x = np.exp(x - n)\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "# feed data into placeholders\n",
        "def feed_dict(x, y, batch_size = 64):\n",
        "    feed = {}\n",
        "    \n",
        "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
        "    \n",
        "    for i in range(input_seq_len):\n",
        "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
        "        \n",
        "    for i in range(output_seq_len):\n",
        "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
        "        \n",
        "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = fr_word2idx['<pad>'], dtype = np.int32)\n",
        "    \n",
        "    for i in range(output_seq_len-1):\n",
        "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
        "        target = feed[decoder_inputs[i+1].name]\n",
        "        for j in range(batch_size):\n",
        "            if target[j] == fr_word2idx['<pad>']:\n",
        "                batch_weights[j] = 0.0\n",
        "        feed[target_weights[i].name] = batch_weights\n",
        "        \n",
        "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
        "    \n",
        "    return feed\n",
        "\n",
        "# decode output sequence\n",
        "def decode_output(output_seq):\n",
        "    words = []\n",
        "    for i in range(output_seq_len):\n",
        "        smax = softmax(output_seq[i])\n",
        "        idx = np.argmax(smax)\n",
        "        words.append(fr_idx2word[idx])\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SjZv7-avu4hs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Config hyperparametes"
      ]
    },
    {
      "metadata": {
        "id": "Kwpi6Ms-u_r-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ops and hyperparameters\n",
        "learning_rate = 5e-3\n",
        "batch_size = 64\n",
        "steps = 10000\n",
        "\n",
        "# ops for projecting outputs\n",
        "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
        "\n",
        "# training optimizer\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "# init op\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# forward step\n",
        "def forward_step(sess, feed):\n",
        "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
        "    return output_sequences\n",
        "\n",
        "# training step\n",
        "def backward_step(sess, feed):\n",
        "    sess.run(optimizer, feed_dict = feed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sRB2enLPvvVr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train!"
      ]
    },
    {
      "metadata": {
        "id": "kPNUMrWLvxAq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 44514
        },
        "outputId": "5993d559-c474-4412-95d9-e76cd0dbbda5"
      },
      "cell_type": "code",
      "source": [
        "# we will use this list to plot losses through steps\n",
        "losses = []\n",
        "\n",
        "# save a checkpoint so we can restore the model later \n",
        "saver = tf.train.Saver()\n",
        "\n",
        "print('------------------TRAINING------------------')\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    t = time.time()\n",
        "    for step in range(steps):\n",
        "        feed = feed_dict(X_train, Y_train)\n",
        "            \n",
        "        backward_step(sess, feed)\n",
        "        \n",
        "        if step % 5 == 4 or step == 0:\n",
        "            loss_value = sess.run(loss, feed_dict = feed)\n",
        "            print('step: {}, loss: {}'.format(step, loss_value))\n",
        "            losses.append(loss_value)\n",
        "        \n",
        "        if step % 20 == 19:\n",
        "            saver.save(sess, 'checkpoints/', global_step=step)\n",
        "            print('Checkpoint is saved')\n",
        "            \n",
        "    print('Training time for {} steps: {}s'.format(steps, time.time() - t))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------TRAINING------------------\n",
            "step: 0, loss: 9.044931411743164\n",
            "step: 4, loss: 8.902560234069824\n",
            "step: 9, loss: 9.12115478515625\n",
            "step: 14, loss: 9.174078941345215\n",
            "step: 19, loss: 9.035680770874023\n",
            "Checkpoint is saved\n",
            "step: 24, loss: 9.14925765991211\n",
            "step: 29, loss: 8.88407039642334\n",
            "step: 34, loss: 8.752191543579102\n",
            "step: 39, loss: 8.639131546020508\n",
            "Checkpoint is saved\n",
            "step: 44, loss: 7.76909065246582\n",
            "step: 49, loss: 7.430342674255371\n",
            "step: 54, loss: 7.341316223144531\n",
            "step: 59, loss: 6.778372764587402\n",
            "Checkpoint is saved\n",
            "step: 64, loss: 6.275045871734619\n",
            "step: 69, loss: 6.146007537841797\n",
            "step: 74, loss: 6.923367500305176\n",
            "step: 79, loss: 5.993732452392578\n",
            "Checkpoint is saved\n",
            "step: 84, loss: 6.3560991287231445\n",
            "step: 89, loss: 5.820133209228516\n",
            "step: 94, loss: 6.606657981872559\n",
            "step: 99, loss: 5.659384727478027\n",
            "Checkpoint is saved\n",
            "step: 104, loss: 5.779996871948242\n",
            "step: 109, loss: 5.6321330070495605\n",
            "step: 114, loss: 6.300575256347656\n",
            "step: 119, loss: 5.134164810180664\n",
            "Checkpoint is saved\n",
            "step: 124, loss: 5.009075164794922\n",
            "step: 129, loss: 5.209165573120117\n",
            "step: 134, loss: 5.83547306060791\n",
            "step: 139, loss: 5.401177406311035\n",
            "Checkpoint is saved\n",
            "step: 144, loss: 5.081613063812256\n",
            "step: 149, loss: 4.713353157043457\n",
            "step: 154, loss: 4.586554527282715\n",
            "step: 159, loss: 5.241584777832031\n",
            "Checkpoint is saved\n",
            "step: 164, loss: 4.446885108947754\n",
            "step: 169, loss: 4.532292366027832\n",
            "step: 174, loss: 4.561502933502197\n",
            "step: 179, loss: 4.59232759475708\n",
            "Checkpoint is saved\n",
            "step: 184, loss: 4.548041343688965\n",
            "step: 189, loss: 4.632739543914795\n",
            "step: 194, loss: 4.985842704772949\n",
            "step: 199, loss: 4.2948713302612305\n",
            "Checkpoint is saved\n",
            "step: 204, loss: 4.520103931427002\n",
            "step: 209, loss: 4.221244812011719\n",
            "step: 214, loss: 4.38236665725708\n",
            "step: 219, loss: 3.9830241203308105\n",
            "Checkpoint is saved\n",
            "step: 224, loss: 4.385674476623535\n",
            "step: 229, loss: 4.130660057067871\n",
            "step: 234, loss: 3.9112038612365723\n",
            "step: 239, loss: 3.8387417793273926\n",
            "Checkpoint is saved\n",
            "step: 244, loss: 3.9155220985412598\n",
            "step: 249, loss: 4.327432155609131\n",
            "step: 254, loss: 3.515584945678711\n",
            "step: 259, loss: 3.9830703735351562\n",
            "Checkpoint is saved\n",
            "step: 264, loss: 4.002020835876465\n",
            "step: 269, loss: 3.8366756439208984\n",
            "step: 274, loss: 3.628911018371582\n",
            "step: 279, loss: 3.529284954071045\n",
            "Checkpoint is saved\n",
            "step: 284, loss: 3.5943026542663574\n",
            "step: 289, loss: 3.5873446464538574\n",
            "step: 294, loss: 3.1433753967285156\n",
            "step: 299, loss: 3.1442971229553223\n",
            "Checkpoint is saved\n",
            "step: 304, loss: 3.53725266456604\n",
            "step: 309, loss: 3.2777414321899414\n",
            "step: 314, loss: 3.5993266105651855\n",
            "step: 319, loss: 3.2642691135406494\n",
            "Checkpoint is saved\n",
            "step: 324, loss: 3.2503113746643066\n",
            "step: 329, loss: 3.3385210037231445\n",
            "step: 334, loss: 2.9486703872680664\n",
            "step: 339, loss: 3.077003002166748\n",
            "Checkpoint is saved\n",
            "step: 344, loss: 3.092297077178955\n",
            "step: 349, loss: 2.807584762573242\n",
            "step: 354, loss: 3.1890060901641846\n",
            "step: 359, loss: 3.1502938270568848\n",
            "Checkpoint is saved\n",
            "step: 364, loss: 3.1484761238098145\n",
            "step: 369, loss: 3.320913076400757\n",
            "step: 374, loss: 2.6080570220947266\n",
            "step: 379, loss: 3.0710508823394775\n",
            "Checkpoint is saved\n",
            "step: 384, loss: 3.0055809020996094\n",
            "step: 389, loss: 2.9840590953826904\n",
            "step: 394, loss: 2.927978038787842\n",
            "step: 399, loss: 2.452929973602295\n",
            "Checkpoint is saved\n",
            "step: 404, loss: 3.017524480819702\n",
            "step: 409, loss: 2.609971046447754\n",
            "step: 414, loss: 2.523552894592285\n",
            "step: 419, loss: 2.8924851417541504\n",
            "Checkpoint is saved\n",
            "step: 424, loss: 2.6314191818237305\n",
            "step: 429, loss: 2.8025803565979004\n",
            "step: 434, loss: 2.921943426132202\n",
            "step: 439, loss: 2.6642508506774902\n",
            "Checkpoint is saved\n",
            "step: 444, loss: 2.568302631378174\n",
            "step: 449, loss: 2.720559597015381\n",
            "step: 454, loss: 2.951892375946045\n",
            "step: 459, loss: 2.6928696632385254\n",
            "Checkpoint is saved\n",
            "step: 464, loss: 2.7237844467163086\n",
            "step: 469, loss: 2.591229200363159\n",
            "step: 474, loss: 2.720090866088867\n",
            "step: 479, loss: 2.780329465866089\n",
            "Checkpoint is saved\n",
            "step: 484, loss: 2.753702163696289\n",
            "step: 489, loss: 2.358539342880249\n",
            "step: 494, loss: 2.4065651893615723\n",
            "step: 499, loss: 2.4486236572265625\n",
            "Checkpoint is saved\n",
            "step: 504, loss: 2.6834964752197266\n",
            "step: 509, loss: 2.5957322120666504\n",
            "step: 514, loss: 2.342954635620117\n",
            "step: 519, loss: 2.2056660652160645\n",
            "Checkpoint is saved\n",
            "step: 524, loss: 2.5247745513916016\n",
            "step: 529, loss: 2.5303149223327637\n",
            "step: 534, loss: 2.4962592124938965\n",
            "step: 539, loss: 2.4038705825805664\n",
            "Checkpoint is saved\n",
            "step: 544, loss: 2.3555264472961426\n",
            "step: 549, loss: 2.379166603088379\n",
            "step: 554, loss: 2.2071287631988525\n",
            "step: 559, loss: 2.1925129890441895\n",
            "Checkpoint is saved\n",
            "step: 564, loss: 2.0948781967163086\n",
            "step: 569, loss: 2.1835460662841797\n",
            "step: 574, loss: 2.3104934692382812\n",
            "step: 579, loss: 2.2551300525665283\n",
            "Checkpoint is saved\n",
            "step: 584, loss: 2.1237521171569824\n",
            "step: 589, loss: 2.1568048000335693\n",
            "step: 594, loss: 2.2300920486450195\n",
            "step: 599, loss: 2.0768556594848633\n",
            "Checkpoint is saved\n",
            "step: 604, loss: 2.392580986022949\n",
            "step: 609, loss: 2.2411046028137207\n",
            "step: 614, loss: 2.1587018966674805\n",
            "step: 619, loss: 2.2002992630004883\n",
            "Checkpoint is saved\n",
            "step: 624, loss: 2.0235204696655273\n",
            "step: 629, loss: 2.277801275253296\n",
            "step: 634, loss: 2.216987371444702\n",
            "step: 639, loss: 2.3215157985687256\n",
            "Checkpoint is saved\n",
            "step: 644, loss: 1.9609394073486328\n",
            "step: 649, loss: 1.8309907913208008\n",
            "step: 654, loss: 1.9857866764068604\n",
            "step: 659, loss: 1.8797898292541504\n",
            "Checkpoint is saved\n",
            "step: 664, loss: 1.9825129508972168\n",
            "step: 669, loss: 2.333159923553467\n",
            "step: 674, loss: 2.501444101333618\n",
            "step: 679, loss: 2.1266117095947266\n",
            "Checkpoint is saved\n",
            "step: 684, loss: 1.8757741451263428\n",
            "step: 689, loss: 2.214484453201294\n",
            "step: 694, loss: 2.1845710277557373\n",
            "step: 699, loss: 1.901869773864746\n",
            "Checkpoint is saved\n",
            "step: 704, loss: 2.0378191471099854\n",
            "step: 709, loss: 1.727568507194519\n",
            "step: 714, loss: 1.7748665809631348\n",
            "step: 719, loss: 1.8220677375793457\n",
            "Checkpoint is saved\n",
            "step: 724, loss: 2.2180352210998535\n",
            "step: 729, loss: 1.907075047492981\n",
            "step: 734, loss: 2.175136089324951\n",
            "step: 739, loss: 1.9238061904907227\n",
            "Checkpoint is saved\n",
            "step: 744, loss: 1.8545653820037842\n",
            "step: 749, loss: 1.8289721012115479\n",
            "step: 754, loss: 2.0631778240203857\n",
            "step: 759, loss: 1.7004692554473877\n",
            "Checkpoint is saved\n",
            "step: 764, loss: 1.68149995803833\n",
            "step: 769, loss: 1.6590750217437744\n",
            "step: 774, loss: 1.9791661500930786\n",
            "step: 779, loss: 1.739593267440796\n",
            "Checkpoint is saved\n",
            "step: 784, loss: 1.7943307161331177\n",
            "step: 789, loss: 1.5875260829925537\n",
            "step: 794, loss: 1.9916614294052124\n",
            "step: 799, loss: 2.0043230056762695\n",
            "Checkpoint is saved\n",
            "step: 804, loss: 1.6049762964248657\n",
            "step: 809, loss: 1.9968054294586182\n",
            "step: 814, loss: 1.5252721309661865\n",
            "step: 819, loss: 1.504206657409668\n",
            "Checkpoint is saved\n",
            "step: 824, loss: 1.9140477180480957\n",
            "step: 829, loss: 1.8579018115997314\n",
            "step: 834, loss: 1.8040287494659424\n",
            "step: 839, loss: 1.7842769622802734\n",
            "Checkpoint is saved\n",
            "step: 844, loss: 1.7219641208648682\n",
            "step: 849, loss: 1.9506183862686157\n",
            "step: 854, loss: 1.9687176942825317\n",
            "step: 859, loss: 1.6889381408691406\n",
            "Checkpoint is saved\n",
            "step: 864, loss: 1.6874971389770508\n",
            "step: 869, loss: 1.5461146831512451\n",
            "step: 874, loss: 1.652980089187622\n",
            "step: 879, loss: 2.151179313659668\n",
            "Checkpoint is saved\n",
            "step: 884, loss: 1.772085428237915\n",
            "step: 889, loss: 1.7100000381469727\n",
            "step: 894, loss: 1.7368767261505127\n",
            "step: 899, loss: 1.4943629503250122\n",
            "Checkpoint is saved\n",
            "step: 904, loss: 1.513770341873169\n",
            "step: 909, loss: 1.6405885219573975\n",
            "step: 914, loss: 1.6981863975524902\n",
            "step: 919, loss: 1.701657772064209\n",
            "Checkpoint is saved\n",
            "step: 924, loss: 1.8550777435302734\n",
            "step: 929, loss: 1.394253134727478\n",
            "step: 934, loss: 1.8451197147369385\n",
            "step: 939, loss: 1.835541009902954\n",
            "Checkpoint is saved\n",
            "step: 944, loss: 1.597058892250061\n",
            "step: 949, loss: 1.8362149000167847\n",
            "step: 954, loss: 1.6702183485031128\n",
            "step: 959, loss: 1.555932879447937\n",
            "Checkpoint is saved\n",
            "step: 964, loss: 1.3751806020736694\n",
            "step: 969, loss: 1.5893135070800781\n",
            "step: 974, loss: 1.4039196968078613\n",
            "step: 979, loss: 1.432963252067566\n",
            "Checkpoint is saved\n",
            "step: 984, loss: 1.2417066097259521\n",
            "step: 989, loss: 1.3467984199523926\n",
            "step: 994, loss: 1.5388336181640625\n",
            "step: 999, loss: 1.4211241006851196\n",
            "Checkpoint is saved\n",
            "step: 1004, loss: 1.231889247894287\n",
            "step: 1009, loss: 1.5655667781829834\n",
            "step: 1014, loss: 1.5443768501281738\n",
            "step: 1019, loss: 1.4643754959106445\n",
            "Checkpoint is saved\n",
            "step: 1024, loss: 1.5955710411071777\n",
            "step: 1029, loss: 1.4318280220031738\n",
            "step: 1034, loss: 1.595859169960022\n",
            "step: 1039, loss: 1.2663140296936035\n",
            "Checkpoint is saved\n",
            "step: 1044, loss: 1.3981846570968628\n",
            "step: 1049, loss: 1.5353012084960938\n",
            "step: 1054, loss: 1.1475129127502441\n",
            "step: 1059, loss: 1.6396323442459106\n",
            "Checkpoint is saved\n",
            "step: 1064, loss: 1.597240686416626\n",
            "step: 1069, loss: 1.3287076950073242\n",
            "step: 1074, loss: 1.6297492980957031\n",
            "step: 1079, loss: 1.5015058517456055\n",
            "Checkpoint is saved\n",
            "step: 1084, loss: 1.232406497001648\n",
            "step: 1089, loss: 1.4962612390518188\n",
            "step: 1094, loss: 1.5793910026550293\n",
            "step: 1099, loss: 1.6253890991210938\n",
            "Checkpoint is saved\n",
            "step: 1104, loss: 1.305490493774414\n",
            "step: 1109, loss: 1.4094829559326172\n",
            "step: 1114, loss: 1.1771578788757324\n",
            "step: 1119, loss: 1.2392706871032715\n",
            "Checkpoint is saved\n",
            "step: 1124, loss: 1.1850513219833374\n",
            "step: 1129, loss: 1.5093235969543457\n",
            "step: 1134, loss: 1.1550521850585938\n",
            "step: 1139, loss: 1.359423041343689\n",
            "Checkpoint is saved\n",
            "step: 1144, loss: 1.318389892578125\n",
            "step: 1149, loss: 1.4315075874328613\n",
            "step: 1154, loss: 1.4803614616394043\n",
            "step: 1159, loss: 1.3616697788238525\n",
            "Checkpoint is saved\n",
            "step: 1164, loss: 1.4151115417480469\n",
            "step: 1169, loss: 1.2800977230072021\n",
            "step: 1174, loss: 1.1553560495376587\n",
            "step: 1179, loss: 1.465142846107483\n",
            "Checkpoint is saved\n",
            "step: 1184, loss: 1.5523655414581299\n",
            "step: 1189, loss: 1.4507079124450684\n",
            "step: 1194, loss: 1.116988182067871\n",
            "step: 1199, loss: 1.1192245483398438\n",
            "Checkpoint is saved\n",
            "step: 1204, loss: 1.397971510887146\n",
            "step: 1209, loss: 1.377404808998108\n",
            "step: 1214, loss: 0.9429572820663452\n",
            "step: 1219, loss: 1.2881453037261963\n",
            "Checkpoint is saved\n",
            "step: 1224, loss: 1.3541724681854248\n",
            "step: 1229, loss: 1.5001404285430908\n",
            "step: 1234, loss: 1.1227681636810303\n",
            "step: 1239, loss: 1.1438875198364258\n",
            "Checkpoint is saved\n",
            "step: 1244, loss: 1.226806640625\n",
            "step: 1249, loss: 1.4449570178985596\n",
            "step: 1254, loss: 1.249324083328247\n",
            "step: 1259, loss: 1.1568759679794312\n",
            "Checkpoint is saved\n",
            "step: 1264, loss: 1.0811715126037598\n",
            "step: 1269, loss: 1.4073102474212646\n",
            "step: 1274, loss: 1.1503510475158691\n",
            "step: 1279, loss: 1.2798070907592773\n",
            "Checkpoint is saved\n",
            "step: 1284, loss: 1.266371488571167\n",
            "step: 1289, loss: 1.1358542442321777\n",
            "step: 1294, loss: 1.2009369134902954\n",
            "step: 1299, loss: 1.1808359622955322\n",
            "Checkpoint is saved\n",
            "step: 1304, loss: 1.030777931213379\n",
            "step: 1309, loss: 0.9755677580833435\n",
            "step: 1314, loss: 1.2236653566360474\n",
            "step: 1319, loss: 1.3301429748535156\n",
            "Checkpoint is saved\n",
            "step: 1324, loss: 1.2772923707962036\n",
            "step: 1329, loss: 0.9408144950866699\n",
            "step: 1334, loss: 1.3555957078933716\n",
            "step: 1339, loss: 0.7967873811721802\n",
            "Checkpoint is saved\n",
            "step: 1344, loss: 1.269425392150879\n",
            "step: 1349, loss: 0.9939204454421997\n",
            "step: 1354, loss: 1.1727558374404907\n",
            "step: 1359, loss: 1.1863420009613037\n",
            "Checkpoint is saved\n",
            "step: 1364, loss: 1.181363582611084\n",
            "step: 1369, loss: 1.0872914791107178\n",
            "step: 1374, loss: 1.1374956369400024\n",
            "step: 1379, loss: 1.156447172164917\n",
            "Checkpoint is saved\n",
            "step: 1384, loss: 0.9944502115249634\n",
            "step: 1389, loss: 1.0971418619155884\n",
            "step: 1394, loss: 1.4527912139892578\n",
            "step: 1399, loss: 1.3703807592391968\n",
            "Checkpoint is saved\n",
            "step: 1404, loss: 1.1885449886322021\n",
            "step: 1409, loss: 1.321190357208252\n",
            "step: 1414, loss: 1.1339045763015747\n",
            "step: 1419, loss: 1.0297791957855225\n",
            "Checkpoint is saved\n",
            "step: 1424, loss: 0.9738577604293823\n",
            "step: 1429, loss: 0.8767743110656738\n",
            "step: 1434, loss: 1.049411416053772\n",
            "step: 1439, loss: 1.1027686595916748\n",
            "Checkpoint is saved\n",
            "step: 1444, loss: 1.2508163452148438\n",
            "step: 1449, loss: 1.1835849285125732\n",
            "step: 1454, loss: 1.0390509366989136\n",
            "step: 1459, loss: 1.1212880611419678\n",
            "Checkpoint is saved\n",
            "step: 1464, loss: 1.137584924697876\n",
            "step: 1469, loss: 1.1046314239501953\n",
            "step: 1474, loss: 1.009904146194458\n",
            "step: 1479, loss: 0.9674969911575317\n",
            "Checkpoint is saved\n",
            "step: 1484, loss: 1.0523558855056763\n",
            "step: 1489, loss: 1.3178342580795288\n",
            "step: 1494, loss: 1.1603435277938843\n",
            "step: 1499, loss: 0.9252087473869324\n",
            "Checkpoint is saved\n",
            "step: 1504, loss: 1.064664363861084\n",
            "step: 1509, loss: 0.9357771873474121\n",
            "step: 1514, loss: 1.0504319667816162\n",
            "step: 1519, loss: 1.084623098373413\n",
            "Checkpoint is saved\n",
            "step: 1524, loss: 1.1153603792190552\n",
            "step: 1529, loss: 1.0470577478408813\n",
            "step: 1534, loss: 1.202318549156189\n",
            "step: 1539, loss: 1.0033988952636719\n",
            "Checkpoint is saved\n",
            "step: 1544, loss: 1.0041842460632324\n",
            "step: 1549, loss: 1.025937557220459\n",
            "step: 1554, loss: 0.9736394882202148\n",
            "step: 1559, loss: 1.0842748880386353\n",
            "Checkpoint is saved\n",
            "step: 1564, loss: 1.0255738496780396\n",
            "step: 1569, loss: 0.9939520955085754\n",
            "step: 1574, loss: 0.8569528460502625\n",
            "step: 1579, loss: 1.0462393760681152\n",
            "Checkpoint is saved\n",
            "step: 1584, loss: 1.3036572933197021\n",
            "step: 1589, loss: 1.0276029109954834\n",
            "step: 1594, loss: 0.9965728521347046\n",
            "step: 1599, loss: 1.0912245512008667\n",
            "Checkpoint is saved\n",
            "step: 1604, loss: 1.0656497478485107\n",
            "step: 1609, loss: 1.1653311252593994\n",
            "step: 1614, loss: 0.9043163061141968\n",
            "step: 1619, loss: 1.0267369747161865\n",
            "Checkpoint is saved\n",
            "step: 1624, loss: 0.7481726408004761\n",
            "step: 1629, loss: 0.9693812727928162\n",
            "step: 1634, loss: 0.9856518507003784\n",
            "step: 1639, loss: 1.0037285089492798\n",
            "Checkpoint is saved\n",
            "step: 1644, loss: 1.0899590253829956\n",
            "step: 1649, loss: 1.0011128187179565\n",
            "step: 1654, loss: 1.219634771347046\n",
            "step: 1659, loss: 0.8377696871757507\n",
            "Checkpoint is saved\n",
            "step: 1664, loss: 0.9930984377861023\n",
            "step: 1669, loss: 0.9552118182182312\n",
            "step: 1674, loss: 1.0757782459259033\n",
            "step: 1679, loss: 1.1180390119552612\n",
            "Checkpoint is saved\n",
            "step: 1684, loss: 1.1974045038223267\n",
            "step: 1689, loss: 0.8184863328933716\n",
            "step: 1694, loss: 0.8622143268585205\n",
            "step: 1699, loss: 0.945354163646698\n",
            "Checkpoint is saved\n",
            "step: 1704, loss: 1.001161813735962\n",
            "step: 1709, loss: 1.0059614181518555\n",
            "step: 1714, loss: 0.9964433908462524\n",
            "step: 1719, loss: 1.0166771411895752\n",
            "Checkpoint is saved\n",
            "step: 1724, loss: 0.8158043622970581\n",
            "step: 1729, loss: 0.9373322129249573\n",
            "step: 1734, loss: 1.0515233278274536\n",
            "step: 1739, loss: 0.9924986958503723\n",
            "Checkpoint is saved\n",
            "step: 1744, loss: 1.1021674871444702\n",
            "step: 1749, loss: 1.122532606124878\n",
            "step: 1754, loss: 1.1390435695648193\n",
            "step: 1759, loss: 1.189835548400879\n",
            "Checkpoint is saved\n",
            "step: 1764, loss: 1.0285212993621826\n",
            "step: 1769, loss: 0.8592020273208618\n",
            "step: 1774, loss: 1.161232829093933\n",
            "step: 1779, loss: 0.8913148045539856\n",
            "Checkpoint is saved\n",
            "step: 1784, loss: 0.9523295164108276\n",
            "step: 1789, loss: 0.8715356588363647\n",
            "step: 1794, loss: 0.9021809101104736\n",
            "step: 1799, loss: 0.8898496627807617\n",
            "Checkpoint is saved\n",
            "step: 1804, loss: 1.040114164352417\n",
            "step: 1809, loss: 1.0540796518325806\n",
            "step: 1814, loss: 0.9904191493988037\n",
            "step: 1819, loss: 0.8087209463119507\n",
            "Checkpoint is saved\n",
            "step: 1824, loss: 0.9217138886451721\n",
            "step: 1829, loss: 0.9771472215652466\n",
            "step: 1834, loss: 0.8101669549942017\n",
            "step: 1839, loss: 0.8148980736732483\n",
            "Checkpoint is saved\n",
            "step: 1844, loss: 0.8698173761367798\n",
            "step: 1849, loss: 0.9843795299530029\n",
            "step: 1854, loss: 0.7765448093414307\n",
            "step: 1859, loss: 1.025315523147583\n",
            "Checkpoint is saved\n",
            "step: 1864, loss: 0.6870676279067993\n",
            "step: 1869, loss: 1.0232551097869873\n",
            "step: 1874, loss: 0.8043816685676575\n",
            "step: 1879, loss: 0.9677200317382812\n",
            "Checkpoint is saved\n",
            "step: 1884, loss: 0.8807146549224854\n",
            "step: 1889, loss: 0.8271934390068054\n",
            "step: 1894, loss: 0.6182336807250977\n",
            "step: 1899, loss: 0.7615031003952026\n",
            "Checkpoint is saved\n",
            "step: 1904, loss: 0.9888163805007935\n",
            "step: 1909, loss: 0.7804093956947327\n",
            "step: 1914, loss: 0.7918022274971008\n",
            "step: 1919, loss: 0.8200580477714539\n",
            "Checkpoint is saved\n",
            "step: 1924, loss: 0.9259236454963684\n",
            "step: 1929, loss: 0.9876344203948975\n",
            "step: 1934, loss: 0.8701673150062561\n",
            "step: 1939, loss: 0.9853300452232361\n",
            "Checkpoint is saved\n",
            "step: 1944, loss: 0.6312578916549683\n",
            "step: 1949, loss: 0.9375720024108887\n",
            "step: 1954, loss: 0.6789199709892273\n",
            "step: 1959, loss: 0.8502227067947388\n",
            "Checkpoint is saved\n",
            "step: 1964, loss: 0.9392583966255188\n",
            "step: 1969, loss: 0.8669171333312988\n",
            "step: 1974, loss: 0.9191268682479858\n",
            "step: 1979, loss: 0.8106632232666016\n",
            "Checkpoint is saved\n",
            "step: 1984, loss: 0.695835530757904\n",
            "step: 1989, loss: 0.8838820457458496\n",
            "step: 1994, loss: 0.8979254961013794\n",
            "step: 1999, loss: 0.9136933088302612\n",
            "Checkpoint is saved\n",
            "step: 2004, loss: 0.8952758312225342\n",
            "step: 2009, loss: 1.10255765914917\n",
            "step: 2014, loss: 0.8051263689994812\n",
            "step: 2019, loss: 0.8575248718261719\n",
            "Checkpoint is saved\n",
            "step: 2024, loss: 0.9176980257034302\n",
            "step: 2029, loss: 0.7250775098800659\n",
            "step: 2034, loss: 0.8273224234580994\n",
            "step: 2039, loss: 1.1653938293457031\n",
            "Checkpoint is saved\n",
            "step: 2044, loss: 0.8522260189056396\n",
            "step: 2049, loss: 0.8839212656021118\n",
            "step: 2054, loss: 0.5919442176818848\n",
            "step: 2059, loss: 0.8613171577453613\n",
            "Checkpoint is saved\n",
            "step: 2064, loss: 0.9509140253067017\n",
            "step: 2069, loss: 0.9311704635620117\n",
            "step: 2074, loss: 0.956994354724884\n",
            "step: 2079, loss: 0.824259877204895\n",
            "Checkpoint is saved\n",
            "step: 2084, loss: 0.7222073078155518\n",
            "step: 2089, loss: 0.7982953190803528\n",
            "step: 2094, loss: 0.7443569898605347\n",
            "step: 2099, loss: 0.6967113018035889\n",
            "Checkpoint is saved\n",
            "step: 2104, loss: 0.8148958683013916\n",
            "step: 2109, loss: 0.8492417335510254\n",
            "step: 2114, loss: 0.7435540556907654\n",
            "step: 2119, loss: 0.7587660551071167\n",
            "Checkpoint is saved\n",
            "step: 2124, loss: 0.8573923110961914\n",
            "step: 2129, loss: 0.7606762647628784\n",
            "step: 2134, loss: 0.8612001538276672\n",
            "step: 2139, loss: 0.6643542647361755\n",
            "Checkpoint is saved\n",
            "step: 2144, loss: 0.9764055013656616\n",
            "step: 2149, loss: 0.6483445167541504\n",
            "step: 2154, loss: 0.7070679664611816\n",
            "step: 2159, loss: 0.6318923234939575\n",
            "Checkpoint is saved\n",
            "step: 2164, loss: 0.9051104784011841\n",
            "step: 2169, loss: 0.8734734058380127\n",
            "step: 2174, loss: 0.7451181411743164\n",
            "step: 2179, loss: 0.7903649806976318\n",
            "Checkpoint is saved\n",
            "step: 2184, loss: 0.6633976101875305\n",
            "step: 2189, loss: 0.9201599359512329\n",
            "step: 2194, loss: 0.7829545736312866\n",
            "step: 2199, loss: 0.658199667930603\n",
            "Checkpoint is saved\n",
            "step: 2204, loss: 0.6907944679260254\n",
            "step: 2209, loss: 0.6628811955451965\n",
            "step: 2214, loss: 0.8299059867858887\n",
            "step: 2219, loss: 0.9466725587844849\n",
            "Checkpoint is saved\n",
            "step: 2224, loss: 0.8356938362121582\n",
            "step: 2229, loss: 0.7466249465942383\n",
            "step: 2234, loss: 0.8467471599578857\n",
            "step: 2239, loss: 0.8086743950843811\n",
            "Checkpoint is saved\n",
            "step: 2244, loss: 0.5354496836662292\n",
            "step: 2249, loss: 0.7440624237060547\n",
            "step: 2254, loss: 0.5843608975410461\n",
            "step: 2259, loss: 0.6422152519226074\n",
            "Checkpoint is saved\n",
            "step: 2264, loss: 0.7853521704673767\n",
            "step: 2269, loss: 0.9581863880157471\n",
            "step: 2274, loss: 0.7223536372184753\n",
            "step: 2279, loss: 0.6672747731208801\n",
            "Checkpoint is saved\n",
            "step: 2284, loss: 0.7034042477607727\n",
            "step: 2289, loss: 0.8803889751434326\n",
            "step: 2294, loss: 0.8221025466918945\n",
            "step: 2299, loss: 0.6421180963516235\n",
            "Checkpoint is saved\n",
            "step: 2304, loss: 0.6921955943107605\n",
            "step: 2309, loss: 0.8022573590278625\n",
            "step: 2314, loss: 0.783493161201477\n",
            "step: 2319, loss: 0.7219877243041992\n",
            "Checkpoint is saved\n",
            "step: 2324, loss: 0.651823103427887\n",
            "step: 2329, loss: 0.9494192004203796\n",
            "step: 2334, loss: 0.6005138158798218\n",
            "step: 2339, loss: 0.7861931324005127\n",
            "Checkpoint is saved\n",
            "step: 2344, loss: 0.6280836462974548\n",
            "step: 2349, loss: 0.9299347996711731\n",
            "step: 2354, loss: 0.6558849811553955\n",
            "step: 2359, loss: 0.7043222188949585\n",
            "Checkpoint is saved\n",
            "step: 2364, loss: 0.7845557928085327\n",
            "step: 2369, loss: 0.6963737607002258\n",
            "step: 2374, loss: 0.7978212833404541\n",
            "step: 2379, loss: 0.8118513822555542\n",
            "Checkpoint is saved\n",
            "step: 2384, loss: 0.6821646094322205\n",
            "step: 2389, loss: 0.6855611801147461\n",
            "step: 2394, loss: 0.7699828147888184\n",
            "step: 2399, loss: 0.7434203028678894\n",
            "Checkpoint is saved\n",
            "step: 2404, loss: 0.6889015436172485\n",
            "step: 2409, loss: 0.5246386528015137\n",
            "step: 2414, loss: 0.7124017477035522\n",
            "step: 2419, loss: 0.597815215587616\n",
            "Checkpoint is saved\n",
            "step: 2424, loss: 0.8267649412155151\n",
            "step: 2429, loss: 0.7666419148445129\n",
            "step: 2434, loss: 0.6871877908706665\n",
            "step: 2439, loss: 0.7617780566215515\n",
            "Checkpoint is saved\n",
            "step: 2444, loss: 0.813772439956665\n",
            "step: 2449, loss: 0.6413077116012573\n",
            "step: 2454, loss: 0.6387166976928711\n",
            "step: 2459, loss: 0.6705018281936646\n",
            "Checkpoint is saved\n",
            "step: 2464, loss: 0.7135495543479919\n",
            "step: 2469, loss: 0.7821497321128845\n",
            "step: 2474, loss: 0.6683211922645569\n",
            "step: 2479, loss: 0.5997486710548401\n",
            "Checkpoint is saved\n",
            "step: 2484, loss: 0.7611323595046997\n",
            "step: 2489, loss: 0.6615938544273376\n",
            "step: 2494, loss: 0.7299332022666931\n",
            "step: 2499, loss: 0.6815499067306519\n",
            "Checkpoint is saved\n",
            "step: 2504, loss: 0.7713883519172668\n",
            "step: 2509, loss: 0.7522996068000793\n",
            "step: 2514, loss: 0.6991924047470093\n",
            "step: 2519, loss: 0.6607067584991455\n",
            "Checkpoint is saved\n",
            "step: 2524, loss: 0.694992184638977\n",
            "step: 2529, loss: 0.680071234703064\n",
            "step: 2534, loss: 0.5548508167266846\n",
            "step: 2539, loss: 0.6218588352203369\n",
            "Checkpoint is saved\n",
            "step: 2544, loss: 0.794428825378418\n",
            "step: 2549, loss: 0.6506471037864685\n",
            "step: 2554, loss: 0.7741307616233826\n",
            "step: 2559, loss: 0.6587020754814148\n",
            "Checkpoint is saved\n",
            "step: 2564, loss: 0.5840262770652771\n",
            "step: 2569, loss: 0.5865702629089355\n",
            "step: 2574, loss: 0.8685702681541443\n",
            "step: 2579, loss: 0.6717495918273926\n",
            "Checkpoint is saved\n",
            "step: 2584, loss: 0.769716739654541\n",
            "step: 2589, loss: 0.728360116481781\n",
            "step: 2594, loss: 0.5981440544128418\n",
            "step: 2599, loss: 0.6547589302062988\n",
            "Checkpoint is saved\n",
            "step: 2604, loss: 0.6363855600357056\n",
            "step: 2609, loss: 0.7413288950920105\n",
            "step: 2614, loss: 0.7071356773376465\n",
            "step: 2619, loss: 0.6812910437583923\n",
            "Checkpoint is saved\n",
            "step: 2624, loss: 0.784563422203064\n",
            "step: 2629, loss: 0.7502393126487732\n",
            "step: 2634, loss: 0.7677745223045349\n",
            "step: 2639, loss: 0.5417635440826416\n",
            "Checkpoint is saved\n",
            "step: 2644, loss: 0.6535512804985046\n",
            "step: 2649, loss: 0.4816684126853943\n",
            "step: 2654, loss: 0.7988746166229248\n",
            "step: 2659, loss: 0.7477113008499146\n",
            "Checkpoint is saved\n",
            "step: 2664, loss: 0.6471601724624634\n",
            "step: 2669, loss: 0.7477465867996216\n",
            "step: 2674, loss: 0.7108038663864136\n",
            "step: 2679, loss: 0.594133734703064\n",
            "Checkpoint is saved\n",
            "step: 2684, loss: 0.621587872505188\n",
            "step: 2689, loss: 0.5576241612434387\n",
            "step: 2694, loss: 0.6797094345092773\n",
            "step: 2699, loss: 0.5271217226982117\n",
            "Checkpoint is saved\n",
            "step: 2704, loss: 0.6571524143218994\n",
            "step: 2709, loss: 0.6670628190040588\n",
            "step: 2714, loss: 0.6377764344215393\n",
            "step: 2719, loss: 0.7566841244697571\n",
            "Checkpoint is saved\n",
            "step: 2724, loss: 0.6151405572891235\n",
            "step: 2729, loss: 0.6203460097312927\n",
            "step: 2734, loss: 0.6902593970298767\n",
            "step: 2739, loss: 0.5972875952720642\n",
            "Checkpoint is saved\n",
            "step: 2744, loss: 0.8303748369216919\n",
            "step: 2749, loss: 0.6543270945549011\n",
            "step: 2754, loss: 0.7421855926513672\n",
            "step: 2759, loss: 0.6545366644859314\n",
            "Checkpoint is saved\n",
            "step: 2764, loss: 0.5701518058776855\n",
            "step: 2769, loss: 0.7073976397514343\n",
            "step: 2774, loss: 0.7725910544395447\n",
            "step: 2779, loss: 0.691370964050293\n",
            "Checkpoint is saved\n",
            "step: 2784, loss: 0.5912770628929138\n",
            "step: 2789, loss: 0.7799334526062012\n",
            "step: 2794, loss: 0.7738195061683655\n",
            "step: 2799, loss: 0.44493210315704346\n",
            "Checkpoint is saved\n",
            "step: 2804, loss: 0.617732048034668\n",
            "step: 2809, loss: 0.6580480933189392\n",
            "step: 2814, loss: 0.6171794533729553\n",
            "step: 2819, loss: 0.7794716358184814\n",
            "Checkpoint is saved\n",
            "step: 2824, loss: 0.5552264451980591\n",
            "step: 2829, loss: 0.5756590366363525\n",
            "step: 2834, loss: 0.7014148235321045\n",
            "step: 2839, loss: 0.6082474589347839\n",
            "Checkpoint is saved\n",
            "step: 2844, loss: 0.6424003839492798\n",
            "step: 2849, loss: 0.5925859212875366\n",
            "step: 2854, loss: 0.6054338812828064\n",
            "step: 2859, loss: 0.7389472723007202\n",
            "Checkpoint is saved\n",
            "step: 2864, loss: 0.6463719606399536\n",
            "step: 2869, loss: 0.679046630859375\n",
            "step: 2874, loss: 0.7120530605316162\n",
            "step: 2879, loss: 0.6955080032348633\n",
            "Checkpoint is saved\n",
            "step: 2884, loss: 0.7007102966308594\n",
            "step: 2889, loss: 0.6162471175193787\n",
            "step: 2894, loss: 0.7524187564849854\n",
            "step: 2899, loss: 0.6532293558120728\n",
            "Checkpoint is saved\n",
            "step: 2904, loss: 0.7230784296989441\n",
            "step: 2909, loss: 0.6246966123580933\n",
            "step: 2914, loss: 0.7031399011611938\n",
            "step: 2919, loss: 0.5822506546974182\n",
            "Checkpoint is saved\n",
            "step: 2924, loss: 0.7166965007781982\n",
            "step: 2929, loss: 0.5954283475875854\n",
            "step: 2934, loss: 0.738356351852417\n",
            "step: 2939, loss: 0.7621595859527588\n",
            "Checkpoint is saved\n",
            "step: 2944, loss: 0.4225451350212097\n",
            "step: 2949, loss: 0.6747658252716064\n",
            "step: 2954, loss: 0.6270546913146973\n",
            "step: 2959, loss: 0.6550135612487793\n",
            "Checkpoint is saved\n",
            "step: 2964, loss: 0.6995540857315063\n",
            "step: 2969, loss: 0.6918774843215942\n",
            "step: 2974, loss: 0.5459704995155334\n",
            "step: 2979, loss: 0.4403533935546875\n",
            "Checkpoint is saved\n",
            "step: 2984, loss: 0.7074019908905029\n",
            "step: 2989, loss: 0.6081667542457581\n",
            "step: 2994, loss: 0.6417694687843323\n",
            "step: 2999, loss: 0.5750093460083008\n",
            "Checkpoint is saved\n",
            "step: 3004, loss: 0.6824896335601807\n",
            "step: 3009, loss: 0.5844868421554565\n",
            "step: 3014, loss: 0.6376744508743286\n",
            "step: 3019, loss: 0.6557084918022156\n",
            "Checkpoint is saved\n",
            "step: 3024, loss: 0.6353045701980591\n",
            "step: 3029, loss: 0.5712367296218872\n",
            "step: 3034, loss: 0.6812857389450073\n",
            "step: 3039, loss: 0.5315905809402466\n",
            "Checkpoint is saved\n",
            "step: 3044, loss: 0.7236043810844421\n",
            "step: 3049, loss: 0.6281830668449402\n",
            "step: 3054, loss: 0.6031038761138916\n",
            "step: 3059, loss: 0.6086452007293701\n",
            "Checkpoint is saved\n",
            "step: 3064, loss: 0.6667443513870239\n",
            "step: 3069, loss: 0.5169878005981445\n",
            "step: 3074, loss: 0.5409889817237854\n",
            "step: 3079, loss: 0.5958176255226135\n",
            "Checkpoint is saved\n",
            "step: 3084, loss: 0.5747495889663696\n",
            "step: 3089, loss: 0.5887152552604675\n",
            "step: 3094, loss: 0.5105746984481812\n",
            "step: 3099, loss: 0.6576639413833618\n",
            "Checkpoint is saved\n",
            "step: 3104, loss: 0.550211489200592\n",
            "step: 3109, loss: 0.7972927093505859\n",
            "step: 3114, loss: 0.551720380783081\n",
            "step: 3119, loss: 0.5874261856079102\n",
            "Checkpoint is saved\n",
            "step: 3124, loss: 0.6139630675315857\n",
            "step: 3129, loss: 0.4757942259311676\n",
            "step: 3134, loss: 0.5601639151573181\n",
            "step: 3139, loss: 0.7086802124977112\n",
            "Checkpoint is saved\n",
            "step: 3144, loss: 0.5065459609031677\n",
            "step: 3149, loss: 0.6446207165718079\n",
            "step: 3154, loss: 0.6879657506942749\n",
            "step: 3159, loss: 0.4727473855018616\n",
            "Checkpoint is saved\n",
            "step: 3164, loss: 0.5489248633384705\n",
            "step: 3169, loss: 0.5296996831893921\n",
            "step: 3174, loss: 0.658984899520874\n",
            "step: 3179, loss: 0.5169079303741455\n",
            "Checkpoint is saved\n",
            "step: 3184, loss: 0.5752028226852417\n",
            "step: 3189, loss: 0.5427463054656982\n",
            "step: 3194, loss: 0.5780303478240967\n",
            "step: 3199, loss: 0.46875500679016113\n",
            "Checkpoint is saved\n",
            "step: 3204, loss: 0.6149633526802063\n",
            "step: 3209, loss: 0.5513467788696289\n",
            "step: 3214, loss: 0.6598385572433472\n",
            "step: 3219, loss: 0.6096649765968323\n",
            "Checkpoint is saved\n",
            "step: 3224, loss: 0.7342908382415771\n",
            "step: 3229, loss: 0.5063487887382507\n",
            "step: 3234, loss: 0.5650876760482788\n",
            "step: 3239, loss: 0.646827220916748\n",
            "Checkpoint is saved\n",
            "step: 3244, loss: 0.6010738611221313\n",
            "step: 3249, loss: 0.6320714354515076\n",
            "step: 3254, loss: 0.6289748549461365\n",
            "step: 3259, loss: 0.7634748816490173\n",
            "Checkpoint is saved\n",
            "step: 3264, loss: 0.6118687987327576\n",
            "step: 3269, loss: 0.44478195905685425\n",
            "step: 3274, loss: 0.6800284385681152\n",
            "step: 3279, loss: 0.5072380304336548\n",
            "Checkpoint is saved\n",
            "step: 3284, loss: 0.46345457434654236\n",
            "step: 3289, loss: 0.587068498134613\n",
            "step: 3294, loss: 0.5809385776519775\n",
            "step: 3299, loss: 0.7365862727165222\n",
            "Checkpoint is saved\n",
            "step: 3304, loss: 0.4934054911136627\n",
            "step: 3309, loss: 0.5654678344726562\n",
            "step: 3314, loss: 0.5175414681434631\n",
            "step: 3319, loss: 0.5490349531173706\n",
            "Checkpoint is saved\n",
            "step: 3324, loss: 0.5213897228240967\n",
            "step: 3329, loss: 0.6207681894302368\n",
            "step: 3334, loss: 0.7031866312026978\n",
            "step: 3339, loss: 0.47281888127326965\n",
            "Checkpoint is saved\n",
            "step: 3344, loss: 0.5754691958427429\n",
            "step: 3349, loss: 0.6501835584640503\n",
            "step: 3354, loss: 0.501262903213501\n",
            "step: 3359, loss: 0.4609910845756531\n",
            "Checkpoint is saved\n",
            "step: 3364, loss: 0.5063010454177856\n",
            "step: 3369, loss: 0.36971354484558105\n",
            "step: 3374, loss: 0.7100629210472107\n",
            "step: 3379, loss: 0.5066335201263428\n",
            "Checkpoint is saved\n",
            "step: 3384, loss: 0.5754479169845581\n",
            "step: 3389, loss: 0.6476617455482483\n",
            "step: 3394, loss: 0.5513418316841125\n",
            "step: 3399, loss: 0.540888786315918\n",
            "Checkpoint is saved\n",
            "step: 3404, loss: 0.3655954599380493\n",
            "step: 3409, loss: 0.5148904919624329\n",
            "step: 3414, loss: 0.48760735988616943\n",
            "step: 3419, loss: 0.6017844080924988\n",
            "Checkpoint is saved\n",
            "step: 3424, loss: 0.520087718963623\n",
            "step: 3429, loss: 0.4770555794239044\n",
            "step: 3434, loss: 0.6329700946807861\n",
            "step: 3439, loss: 0.6361784934997559\n",
            "Checkpoint is saved\n",
            "step: 3444, loss: 0.6308361291885376\n",
            "step: 3449, loss: 0.599013090133667\n",
            "step: 3454, loss: 0.676404595375061\n",
            "step: 3459, loss: 0.49269992113113403\n",
            "Checkpoint is saved\n",
            "step: 3464, loss: 0.46925026178359985\n",
            "step: 3469, loss: 0.6036816239356995\n",
            "step: 3474, loss: 0.5175433158874512\n",
            "step: 3479, loss: 0.5905447006225586\n",
            "Checkpoint is saved\n",
            "step: 3484, loss: 0.4997290372848511\n",
            "step: 3489, loss: 0.6582364439964294\n",
            "step: 3494, loss: 0.5590441226959229\n",
            "step: 3499, loss: 0.7600264549255371\n",
            "Checkpoint is saved\n",
            "step: 3504, loss: 0.6541045904159546\n",
            "step: 3509, loss: 0.5325652360916138\n",
            "step: 3514, loss: 0.6275979280471802\n",
            "step: 3519, loss: 0.5261117815971375\n",
            "Checkpoint is saved\n",
            "step: 3524, loss: 0.4458027184009552\n",
            "step: 3529, loss: 0.5276603102684021\n",
            "step: 3534, loss: 0.6459740996360779\n",
            "step: 3539, loss: 0.6339343786239624\n",
            "Checkpoint is saved\n",
            "step: 3544, loss: 0.5648183226585388\n",
            "step: 3549, loss: 0.576880693435669\n",
            "step: 3554, loss: 0.6879942417144775\n",
            "step: 3559, loss: 0.5268417596817017\n",
            "Checkpoint is saved\n",
            "step: 3564, loss: 0.36625388264656067\n",
            "step: 3569, loss: 0.5453648567199707\n",
            "step: 3574, loss: 0.517312228679657\n",
            "step: 3579, loss: 0.5651984810829163\n",
            "Checkpoint is saved\n",
            "step: 3584, loss: 0.4290663003921509\n",
            "step: 3589, loss: 0.6171297430992126\n",
            "step: 3594, loss: 0.4837521016597748\n",
            "step: 3599, loss: 0.5467736124992371\n",
            "Checkpoint is saved\n",
            "step: 3604, loss: 0.6700893640518188\n",
            "step: 3609, loss: 0.6561928987503052\n",
            "step: 3614, loss: 0.6020192503929138\n",
            "step: 3619, loss: 0.5386824607849121\n",
            "Checkpoint is saved\n",
            "step: 3624, loss: 0.524703860282898\n",
            "step: 3629, loss: 0.515255868434906\n",
            "step: 3634, loss: 0.5636264681816101\n",
            "step: 3639, loss: 0.6748983860015869\n",
            "Checkpoint is saved\n",
            "step: 3644, loss: 0.4634127616882324\n",
            "step: 3649, loss: 0.49177679419517517\n",
            "step: 3654, loss: 0.6422555446624756\n",
            "step: 3659, loss: 0.4610620141029358\n",
            "Checkpoint is saved\n",
            "step: 3664, loss: 0.46690237522125244\n",
            "step: 3669, loss: 0.48324310779571533\n",
            "step: 3674, loss: 0.5122516751289368\n",
            "step: 3679, loss: 0.5416701436042786\n",
            "Checkpoint is saved\n",
            "step: 3684, loss: 0.4429565668106079\n",
            "step: 3689, loss: 0.4443325698375702\n",
            "step: 3694, loss: 0.6391881108283997\n",
            "step: 3699, loss: 0.49217063188552856\n",
            "Checkpoint is saved\n",
            "step: 3704, loss: 0.56131511926651\n",
            "step: 3709, loss: 0.5576786398887634\n",
            "step: 3714, loss: 0.5906895399093628\n",
            "step: 3719, loss: 0.510663628578186\n",
            "Checkpoint is saved\n",
            "step: 3724, loss: 0.5411890745162964\n",
            "step: 3729, loss: 0.46124374866485596\n",
            "step: 3734, loss: 0.41613781452178955\n",
            "step: 3739, loss: 0.4606019854545593\n",
            "Checkpoint is saved\n",
            "step: 3744, loss: 0.5121859312057495\n",
            "step: 3749, loss: 0.5256590843200684\n",
            "step: 3754, loss: 0.5698090195655823\n",
            "step: 3759, loss: 0.563143253326416\n",
            "Checkpoint is saved\n",
            "step: 3764, loss: 0.5516967177391052\n",
            "step: 3769, loss: 0.42009350657463074\n",
            "step: 3774, loss: 0.5152825117111206\n",
            "step: 3779, loss: 0.5596296787261963\n",
            "Checkpoint is saved\n",
            "step: 3784, loss: 0.5298266410827637\n",
            "step: 3789, loss: 0.6240800619125366\n",
            "step: 3794, loss: 0.5360005497932434\n",
            "step: 3799, loss: 0.5150110721588135\n",
            "Checkpoint is saved\n",
            "step: 3804, loss: 0.6101272106170654\n",
            "step: 3809, loss: 0.473136305809021\n",
            "step: 3814, loss: 0.5225915312767029\n",
            "step: 3819, loss: 0.5656944513320923\n",
            "Checkpoint is saved\n",
            "step: 3824, loss: 0.3754819631576538\n",
            "step: 3829, loss: 0.46193361282348633\n",
            "step: 3834, loss: 0.4925178289413452\n",
            "step: 3839, loss: 0.47511518001556396\n",
            "Checkpoint is saved\n",
            "step: 3844, loss: 0.5773236751556396\n",
            "step: 3849, loss: 0.498012900352478\n",
            "step: 3854, loss: 0.5856426954269409\n",
            "step: 3859, loss: 0.5082600116729736\n",
            "Checkpoint is saved\n",
            "step: 3864, loss: 0.5723073482513428\n",
            "step: 3869, loss: 0.5858916640281677\n",
            "step: 3874, loss: 0.6034839153289795\n",
            "step: 3879, loss: 0.546777606010437\n",
            "Checkpoint is saved\n",
            "step: 3884, loss: 0.45285117626190186\n",
            "step: 3889, loss: 0.5466361045837402\n",
            "step: 3894, loss: 0.5577870607376099\n",
            "step: 3899, loss: 0.5882065296173096\n",
            "Checkpoint is saved\n",
            "step: 3904, loss: 0.584551990032196\n",
            "step: 3909, loss: 0.7316955327987671\n",
            "step: 3914, loss: 0.5325034856796265\n",
            "step: 3919, loss: 0.5960756540298462\n",
            "Checkpoint is saved\n",
            "step: 3924, loss: 0.45960313081741333\n",
            "step: 3929, loss: 0.6990858316421509\n",
            "step: 3934, loss: 0.48112723231315613\n",
            "step: 3939, loss: 0.6431629657745361\n",
            "Checkpoint is saved\n",
            "step: 3944, loss: 0.5455483794212341\n",
            "step: 3949, loss: 0.578557014465332\n",
            "step: 3954, loss: 0.6518195867538452\n",
            "step: 3959, loss: 0.5489993095397949\n",
            "Checkpoint is saved\n",
            "step: 3964, loss: 0.5353505611419678\n",
            "step: 3969, loss: 0.5398208498954773\n",
            "step: 3974, loss: 0.5419541597366333\n",
            "step: 3979, loss: 0.5653927326202393\n",
            "Checkpoint is saved\n",
            "step: 3984, loss: 0.4495173990726471\n",
            "step: 3989, loss: 0.42655128240585327\n",
            "step: 3994, loss: 0.4289330840110779\n",
            "step: 3999, loss: 0.3608683943748474\n",
            "Checkpoint is saved\n",
            "step: 4004, loss: 0.4920237362384796\n",
            "step: 4009, loss: 0.6182215213775635\n",
            "step: 4014, loss: 0.5334926843643188\n",
            "step: 4019, loss: 0.5813449621200562\n",
            "Checkpoint is saved\n",
            "step: 4024, loss: 0.5361163020133972\n",
            "step: 4029, loss: 0.6289626955986023\n",
            "step: 4034, loss: 0.7466045618057251\n",
            "step: 4039, loss: 0.5804327726364136\n",
            "Checkpoint is saved\n",
            "step: 4044, loss: 0.5771254897117615\n",
            "step: 4049, loss: 0.3571662902832031\n",
            "step: 4054, loss: 0.5810137987136841\n",
            "step: 4059, loss: 0.47362834215164185\n",
            "Checkpoint is saved\n",
            "step: 4064, loss: 0.6679731607437134\n",
            "step: 4069, loss: 0.5021745562553406\n",
            "step: 4074, loss: 0.4514981508255005\n",
            "step: 4079, loss: 0.5544736385345459\n",
            "Checkpoint is saved\n",
            "step: 4084, loss: 0.49254459142684937\n",
            "step: 4089, loss: 0.532128095626831\n",
            "step: 4094, loss: 0.4978604316711426\n",
            "step: 4099, loss: 0.5210083723068237\n",
            "Checkpoint is saved\n",
            "step: 4104, loss: 0.6546347737312317\n",
            "step: 4109, loss: 0.5377292633056641\n",
            "step: 4114, loss: 0.49315303564071655\n",
            "step: 4119, loss: 0.6531264781951904\n",
            "Checkpoint is saved\n",
            "step: 4124, loss: 0.42255669832229614\n",
            "step: 4129, loss: 0.6464565396308899\n",
            "step: 4134, loss: 0.6542322635650635\n",
            "step: 4139, loss: 0.46499568223953247\n",
            "Checkpoint is saved\n",
            "step: 4144, loss: 0.5030221939086914\n",
            "step: 4149, loss: 0.5748807191848755\n",
            "step: 4154, loss: 0.4936372637748718\n",
            "step: 4159, loss: 0.3961254954338074\n",
            "Checkpoint is saved\n",
            "step: 4164, loss: 0.5049273371696472\n",
            "step: 4169, loss: 0.4730713367462158\n",
            "step: 4174, loss: 0.6144464612007141\n",
            "step: 4179, loss: 0.4522087574005127\n",
            "Checkpoint is saved\n",
            "step: 4184, loss: 0.574146032333374\n",
            "step: 4189, loss: 0.48397761583328247\n",
            "step: 4194, loss: 0.5727165341377258\n",
            "step: 4199, loss: 0.4323071837425232\n",
            "Checkpoint is saved\n",
            "step: 4204, loss: 0.6119829416275024\n",
            "step: 4209, loss: 0.4831918478012085\n",
            "step: 4214, loss: 0.5344074368476868\n",
            "step: 4219, loss: 0.43764132261276245\n",
            "Checkpoint is saved\n",
            "step: 4224, loss: 0.5124309062957764\n",
            "step: 4229, loss: 0.405105859041214\n",
            "step: 4234, loss: 0.6299861669540405\n",
            "step: 4239, loss: 0.5049126148223877\n",
            "Checkpoint is saved\n",
            "step: 4244, loss: 0.47806236147880554\n",
            "step: 4249, loss: 0.48340725898742676\n",
            "step: 4254, loss: 0.4044013023376465\n",
            "step: 4259, loss: 0.5985515117645264\n",
            "Checkpoint is saved\n",
            "step: 4264, loss: 0.5813775062561035\n",
            "step: 4269, loss: 0.41365599632263184\n",
            "step: 4274, loss: 0.5564284324645996\n",
            "step: 4279, loss: 0.6129283905029297\n",
            "Checkpoint is saved\n",
            "step: 4284, loss: 0.4857165813446045\n",
            "step: 4289, loss: 0.4148273169994354\n",
            "step: 4294, loss: 0.5032120943069458\n",
            "step: 4299, loss: 0.593665599822998\n",
            "Checkpoint is saved\n",
            "step: 4304, loss: 0.42882251739501953\n",
            "step: 4309, loss: 0.5170483589172363\n",
            "step: 4314, loss: 0.5377829074859619\n",
            "step: 4319, loss: 0.5824818015098572\n",
            "Checkpoint is saved\n",
            "step: 4324, loss: 0.4711017608642578\n",
            "step: 4329, loss: 0.5568637847900391\n",
            "step: 4334, loss: 0.4047393202781677\n",
            "step: 4339, loss: 0.5410635471343994\n",
            "Checkpoint is saved\n",
            "step: 4344, loss: 0.4790491759777069\n",
            "step: 4349, loss: 0.3808627724647522\n",
            "step: 4354, loss: 0.5615194439888\n",
            "step: 4359, loss: 0.6259759664535522\n",
            "Checkpoint is saved\n",
            "step: 4364, loss: 0.5251779556274414\n",
            "step: 4369, loss: 0.6804615259170532\n",
            "step: 4374, loss: 0.4037635922431946\n",
            "step: 4379, loss: 0.43258219957351685\n",
            "Checkpoint is saved\n",
            "step: 4384, loss: 0.514403223991394\n",
            "step: 4389, loss: 0.4340983033180237\n",
            "step: 4394, loss: 0.46882015466690063\n",
            "step: 4399, loss: 0.4726182222366333\n",
            "Checkpoint is saved\n",
            "step: 4404, loss: 0.5084348917007446\n",
            "step: 4409, loss: 0.5086446404457092\n",
            "step: 4414, loss: 0.4977152347564697\n",
            "step: 4419, loss: 0.4259053170681\n",
            "Checkpoint is saved\n",
            "step: 4424, loss: 0.40735191106796265\n",
            "step: 4429, loss: 0.3975452780723572\n",
            "step: 4434, loss: 0.4980829656124115\n",
            "step: 4439, loss: 0.3894805908203125\n",
            "Checkpoint is saved\n",
            "step: 4444, loss: 0.3723945617675781\n",
            "step: 4449, loss: 0.36210399866104126\n",
            "step: 4454, loss: 0.48796260356903076\n",
            "step: 4459, loss: 0.6217532753944397\n",
            "Checkpoint is saved\n",
            "step: 4464, loss: 0.567704975605011\n",
            "step: 4469, loss: 0.5325158834457397\n",
            "step: 4474, loss: 0.4803653061389923\n",
            "step: 4479, loss: 0.5525588989257812\n",
            "Checkpoint is saved\n",
            "step: 4484, loss: 0.4127669334411621\n",
            "step: 4489, loss: 0.5190465450286865\n",
            "step: 4494, loss: 0.3355613350868225\n",
            "step: 4499, loss: 0.47807419300079346\n",
            "Checkpoint is saved\n",
            "step: 4504, loss: 0.487396776676178\n",
            "step: 4509, loss: 0.46216505765914917\n",
            "step: 4514, loss: 0.4487968683242798\n",
            "step: 4519, loss: 0.4996621012687683\n",
            "Checkpoint is saved\n",
            "step: 4524, loss: 0.37401023507118225\n",
            "step: 4529, loss: 0.4341644048690796\n",
            "step: 4534, loss: 0.46710366010665894\n",
            "step: 4539, loss: 0.4036441445350647\n",
            "Checkpoint is saved\n",
            "step: 4544, loss: 0.6398657560348511\n",
            "step: 4549, loss: 0.5939856767654419\n",
            "step: 4554, loss: 0.5584707260131836\n",
            "step: 4559, loss: 0.46047765016555786\n",
            "Checkpoint is saved\n",
            "step: 4564, loss: 0.403994619846344\n",
            "step: 4569, loss: 0.351548969745636\n",
            "step: 4574, loss: 0.45830047130584717\n",
            "step: 4579, loss: 0.4506906270980835\n",
            "Checkpoint is saved\n",
            "step: 4584, loss: 0.5280519723892212\n",
            "step: 4589, loss: 0.49264925718307495\n",
            "step: 4594, loss: 0.42509251832962036\n",
            "step: 4599, loss: 0.5956571102142334\n",
            "Checkpoint is saved\n",
            "step: 4604, loss: 0.4105798006057739\n",
            "step: 4609, loss: 0.41660410165786743\n",
            "step: 4614, loss: 0.47715169191360474\n",
            "step: 4619, loss: 0.5661351680755615\n",
            "Checkpoint is saved\n",
            "step: 4624, loss: 0.5209043025970459\n",
            "step: 4629, loss: 0.44336333870887756\n",
            "step: 4634, loss: 0.45066434144973755\n",
            "step: 4639, loss: 0.5266999006271362\n",
            "Checkpoint is saved\n",
            "step: 4644, loss: 0.5244718790054321\n",
            "step: 4649, loss: 0.5458928346633911\n",
            "step: 4654, loss: 0.369489461183548\n",
            "step: 4659, loss: 0.3918144106864929\n",
            "Checkpoint is saved\n",
            "step: 4664, loss: 0.47600552439689636\n",
            "step: 4669, loss: 0.5058180093765259\n",
            "step: 4674, loss: 0.6215280294418335\n",
            "step: 4679, loss: 0.4354715645313263\n",
            "Checkpoint is saved\n",
            "step: 4684, loss: 0.537091851234436\n",
            "step: 4689, loss: 0.48664700984954834\n",
            "step: 4694, loss: 0.5157462954521179\n",
            "step: 4699, loss: 0.5236647129058838\n",
            "Checkpoint is saved\n",
            "step: 4704, loss: 0.5017242431640625\n",
            "step: 4709, loss: 0.44288885593414307\n",
            "step: 4714, loss: 0.36679553985595703\n",
            "step: 4719, loss: 0.5391400456428528\n",
            "Checkpoint is saved\n",
            "step: 4724, loss: 0.4444732069969177\n",
            "step: 4729, loss: 0.5297325849533081\n",
            "step: 4734, loss: 0.4073817729949951\n",
            "step: 4739, loss: 0.4285591244697571\n",
            "Checkpoint is saved\n",
            "step: 4744, loss: 0.5386825799942017\n",
            "step: 4749, loss: 0.45847803354263306\n",
            "step: 4754, loss: 0.5508802533149719\n",
            "step: 4759, loss: 0.42648112773895264\n",
            "Checkpoint is saved\n",
            "step: 4764, loss: 0.4359170198440552\n",
            "step: 4769, loss: 0.5020582675933838\n",
            "step: 4774, loss: 0.36591482162475586\n",
            "step: 4779, loss: 0.39208418130874634\n",
            "Checkpoint is saved\n",
            "step: 4784, loss: 0.47591906785964966\n",
            "step: 4789, loss: 0.5027062892913818\n",
            "step: 4794, loss: 0.4450756311416626\n",
            "step: 4799, loss: 0.4852348268032074\n",
            "Checkpoint is saved\n",
            "step: 4804, loss: 0.49430322647094727\n",
            "step: 4809, loss: 0.41148844361305237\n",
            "step: 4814, loss: 0.4003048241138458\n",
            "step: 4819, loss: 0.43321776390075684\n",
            "Checkpoint is saved\n",
            "step: 4824, loss: 0.4184143543243408\n",
            "step: 4829, loss: 0.5124589800834656\n",
            "step: 4834, loss: 0.45678427815437317\n",
            "step: 4839, loss: 0.4130437970161438\n",
            "Checkpoint is saved\n",
            "step: 4844, loss: 0.4534614086151123\n",
            "step: 4849, loss: 0.4506763219833374\n",
            "step: 4854, loss: 0.48669689893722534\n",
            "step: 4859, loss: 0.5927673578262329\n",
            "Checkpoint is saved\n",
            "step: 4864, loss: 0.33194708824157715\n",
            "step: 4869, loss: 0.5455699563026428\n",
            "step: 4874, loss: 0.44461971521377563\n",
            "step: 4879, loss: 0.5036411285400391\n",
            "Checkpoint is saved\n",
            "step: 4884, loss: 0.5041910409927368\n",
            "step: 4889, loss: 0.5837064385414124\n",
            "step: 4894, loss: 0.3982149064540863\n",
            "step: 4899, loss: 0.4255867004394531\n",
            "Checkpoint is saved\n",
            "step: 4904, loss: 0.5060082674026489\n",
            "step: 4909, loss: 0.5204981565475464\n",
            "step: 4914, loss: 0.5018908381462097\n",
            "step: 4919, loss: 0.5068296194076538\n",
            "Checkpoint is saved\n",
            "step: 4924, loss: 0.3513878285884857\n",
            "step: 4929, loss: 0.5790568590164185\n",
            "step: 4934, loss: 0.3619452714920044\n",
            "step: 4939, loss: 0.4474342167377472\n",
            "Checkpoint is saved\n",
            "step: 4944, loss: 0.37059977650642395\n",
            "step: 4949, loss: 0.6210975646972656\n",
            "step: 4954, loss: 0.3409361243247986\n",
            "step: 4959, loss: 0.501521110534668\n",
            "Checkpoint is saved\n",
            "step: 4964, loss: 0.5158365964889526\n",
            "step: 4969, loss: 0.6134339570999146\n",
            "step: 4974, loss: 0.4884927570819855\n",
            "step: 4979, loss: 0.45550698041915894\n",
            "Checkpoint is saved\n",
            "step: 4984, loss: 0.41169822216033936\n",
            "step: 4989, loss: 0.4702899158000946\n",
            "step: 4994, loss: 0.44601529836654663\n",
            "step: 4999, loss: 0.4633691906929016\n",
            "Checkpoint is saved\n",
            "step: 5004, loss: 0.5407652258872986\n",
            "step: 5009, loss: 0.3805052936077118\n",
            "step: 5014, loss: 0.46423986554145813\n",
            "step: 5019, loss: 0.47944098711013794\n",
            "Checkpoint is saved\n",
            "step: 5024, loss: 0.32102829217910767\n",
            "step: 5029, loss: 0.5240634679794312\n",
            "step: 5034, loss: 0.43373948335647583\n",
            "step: 5039, loss: 0.4807724356651306\n",
            "Checkpoint is saved\n",
            "step: 5044, loss: 0.5313683748245239\n",
            "step: 5049, loss: 0.4069655239582062\n",
            "step: 5054, loss: 0.4149128496646881\n",
            "step: 5059, loss: 0.4403526186943054\n",
            "Checkpoint is saved\n",
            "step: 5064, loss: 0.4573793113231659\n",
            "step: 5069, loss: 0.5678538680076599\n",
            "step: 5074, loss: 0.4692128002643585\n",
            "step: 5079, loss: 0.4465027451515198\n",
            "Checkpoint is saved\n",
            "step: 5084, loss: 0.5343954563140869\n",
            "step: 5089, loss: 0.44817858934402466\n",
            "step: 5094, loss: 0.49809369444847107\n",
            "step: 5099, loss: 0.319868266582489\n",
            "Checkpoint is saved\n",
            "step: 5104, loss: 0.5930072665214539\n",
            "step: 5109, loss: 0.5193566083908081\n",
            "step: 5114, loss: 0.43048036098480225\n",
            "step: 5119, loss: 0.40580952167510986\n",
            "Checkpoint is saved\n",
            "step: 5124, loss: 0.4461827874183655\n",
            "step: 5129, loss: 0.45659008622169495\n",
            "step: 5134, loss: 0.34886619448661804\n",
            "step: 5139, loss: 0.3755720257759094\n",
            "Checkpoint is saved\n",
            "step: 5144, loss: 0.39700305461883545\n",
            "step: 5149, loss: 0.45841723680496216\n",
            "step: 5154, loss: 0.4480780065059662\n",
            "step: 5159, loss: 0.5087215900421143\n",
            "Checkpoint is saved\n",
            "step: 5164, loss: 0.47747284173965454\n",
            "step: 5169, loss: 0.4679451286792755\n",
            "step: 5174, loss: 0.391303151845932\n",
            "step: 5179, loss: 0.44405829906463623\n",
            "Checkpoint is saved\n",
            "step: 5184, loss: 0.3633601665496826\n",
            "step: 5189, loss: 0.5269775390625\n",
            "step: 5194, loss: 0.36798134446144104\n",
            "step: 5199, loss: 0.3678392469882965\n",
            "Checkpoint is saved\n",
            "step: 5204, loss: 0.3674236238002777\n",
            "step: 5209, loss: 0.45819419622421265\n",
            "step: 5214, loss: 0.4655115008354187\n",
            "step: 5219, loss: 0.5022437572479248\n",
            "Checkpoint is saved\n",
            "step: 5224, loss: 0.3675740361213684\n",
            "step: 5229, loss: 0.49166321754455566\n",
            "step: 5234, loss: 0.4670228958129883\n",
            "step: 5239, loss: 0.4450773000717163\n",
            "Checkpoint is saved\n",
            "step: 5244, loss: 0.35447078943252563\n",
            "step: 5249, loss: 0.38130998611450195\n",
            "step: 5254, loss: 0.5220725536346436\n",
            "step: 5259, loss: 0.38081973791122437\n",
            "Checkpoint is saved\n",
            "step: 5264, loss: 0.4938943386077881\n",
            "step: 5269, loss: 0.40365567803382874\n",
            "step: 5274, loss: 0.37326422333717346\n",
            "step: 5279, loss: 0.4847486913204193\n",
            "Checkpoint is saved\n",
            "step: 5284, loss: 0.40327590703964233\n",
            "step: 5289, loss: 0.3947634696960449\n",
            "step: 5294, loss: 0.4056944251060486\n",
            "step: 5299, loss: 0.5677870512008667\n",
            "Checkpoint is saved\n",
            "step: 5304, loss: 0.5488073825836182\n",
            "step: 5309, loss: 0.48025327920913696\n",
            "step: 5314, loss: 0.5077254176139832\n",
            "step: 5319, loss: 0.3818258047103882\n",
            "Checkpoint is saved\n",
            "step: 5324, loss: 0.4840984344482422\n",
            "step: 5329, loss: 0.49926093220710754\n",
            "step: 5334, loss: 0.45587465167045593\n",
            "step: 5339, loss: 0.5685930848121643\n",
            "Checkpoint is saved\n",
            "step: 5344, loss: 0.40511175990104675\n",
            "step: 5349, loss: 0.36786365509033203\n",
            "step: 5354, loss: 0.45335450768470764\n",
            "step: 5359, loss: 0.4573018550872803\n",
            "Checkpoint is saved\n",
            "step: 5364, loss: 0.5494731068611145\n",
            "step: 5369, loss: 0.45677393674850464\n",
            "step: 5374, loss: 0.40344420075416565\n",
            "step: 5379, loss: 0.39270085096359253\n",
            "Checkpoint is saved\n",
            "step: 5384, loss: 0.412106454372406\n",
            "step: 5389, loss: 0.41757506132125854\n",
            "step: 5394, loss: 0.4465213716030121\n",
            "step: 5399, loss: 0.4027154743671417\n",
            "Checkpoint is saved\n",
            "step: 5404, loss: 0.49768638610839844\n",
            "step: 5409, loss: 0.5014681816101074\n",
            "step: 5414, loss: 0.39713817834854126\n",
            "step: 5419, loss: 0.552791953086853\n",
            "Checkpoint is saved\n",
            "step: 5424, loss: 0.37708503007888794\n",
            "step: 5429, loss: 0.4537074863910675\n",
            "step: 5434, loss: 0.4350413978099823\n",
            "step: 5439, loss: 0.4028790593147278\n",
            "Checkpoint is saved\n",
            "step: 5444, loss: 0.48521339893341064\n",
            "step: 5449, loss: 0.5530247688293457\n",
            "step: 5454, loss: 0.4982556998729706\n",
            "step: 5459, loss: 0.47989359498023987\n",
            "Checkpoint is saved\n",
            "step: 5464, loss: 0.4598427712917328\n",
            "step: 5469, loss: 0.38745445013046265\n",
            "step: 5474, loss: 0.4383680522441864\n",
            "step: 5479, loss: 0.5094765424728394\n",
            "Checkpoint is saved\n",
            "step: 5484, loss: 0.43268853425979614\n",
            "step: 5489, loss: 0.4518106281757355\n",
            "step: 5494, loss: 0.4440087080001831\n",
            "step: 5499, loss: 0.499826043844223\n",
            "Checkpoint is saved\n",
            "step: 5504, loss: 0.4943510591983795\n",
            "step: 5509, loss: 0.48123568296432495\n",
            "step: 5514, loss: 0.4076347053050995\n",
            "step: 5519, loss: 0.3806629180908203\n",
            "Checkpoint is saved\n",
            "step: 5524, loss: 0.5156861543655396\n",
            "step: 5529, loss: 0.29725784063339233\n",
            "step: 5534, loss: 0.3788577914237976\n",
            "step: 5539, loss: 0.379849374294281\n",
            "Checkpoint is saved\n",
            "step: 5544, loss: 0.4298979640007019\n",
            "step: 5549, loss: 0.4941018223762512\n",
            "step: 5554, loss: 0.4478696584701538\n",
            "step: 5559, loss: 0.4096492528915405\n",
            "Checkpoint is saved\n",
            "step: 5564, loss: 0.48779964447021484\n",
            "step: 5569, loss: 0.5223613977432251\n",
            "step: 5574, loss: 0.45271238684654236\n",
            "step: 5579, loss: 0.3815534710884094\n",
            "Checkpoint is saved\n",
            "step: 5584, loss: 0.42106831073760986\n",
            "step: 5589, loss: 0.5030767917633057\n",
            "step: 5594, loss: 0.44607385993003845\n",
            "step: 5599, loss: 0.4346538186073303\n",
            "Checkpoint is saved\n",
            "step: 5604, loss: 0.3664836287498474\n",
            "step: 5609, loss: 0.45020562410354614\n",
            "step: 5614, loss: 0.45770877599716187\n",
            "step: 5619, loss: 0.4491792321205139\n",
            "Checkpoint is saved\n",
            "step: 5624, loss: 0.4136000871658325\n",
            "step: 5629, loss: 0.4666486382484436\n",
            "step: 5634, loss: 0.44205784797668457\n",
            "step: 5639, loss: 0.310595840215683\n",
            "Checkpoint is saved\n",
            "step: 5644, loss: 0.3905797600746155\n",
            "step: 5649, loss: 0.4081336259841919\n",
            "step: 5654, loss: 0.41504043340682983\n",
            "step: 5659, loss: 0.3953334093093872\n",
            "Checkpoint is saved\n",
            "step: 5664, loss: 0.44396770000457764\n",
            "step: 5669, loss: 0.529857337474823\n",
            "step: 5674, loss: 0.354847252368927\n",
            "step: 5679, loss: 0.3886101543903351\n",
            "Checkpoint is saved\n",
            "step: 5684, loss: 0.47551673650741577\n",
            "step: 5689, loss: 0.468267560005188\n",
            "step: 5694, loss: 0.42912212014198303\n",
            "step: 5699, loss: 0.5267547965049744\n",
            "Checkpoint is saved\n",
            "step: 5704, loss: 0.41129910945892334\n",
            "step: 5709, loss: 0.4032169580459595\n",
            "step: 5714, loss: 0.5458021759986877\n",
            "step: 5719, loss: 0.4197126030921936\n",
            "Checkpoint is saved\n",
            "step: 5724, loss: 0.36814770102500916\n",
            "step: 5729, loss: 0.42674556374549866\n",
            "step: 5734, loss: 0.3896225094795227\n",
            "step: 5739, loss: 0.44582587480545044\n",
            "Checkpoint is saved\n",
            "step: 5744, loss: 0.3019639849662781\n",
            "step: 5749, loss: 0.38026508688926697\n",
            "step: 5754, loss: 0.4809100329875946\n",
            "step: 5759, loss: 0.34110888838768005\n",
            "Checkpoint is saved\n",
            "step: 5764, loss: 0.5604218244552612\n",
            "step: 5769, loss: 0.3793102502822876\n",
            "step: 5774, loss: 0.38085952401161194\n",
            "step: 5779, loss: 0.36937403678894043\n",
            "Checkpoint is saved\n",
            "step: 5784, loss: 0.4764758050441742\n",
            "step: 5789, loss: 0.45679304003715515\n",
            "step: 5794, loss: 0.3345315456390381\n",
            "step: 5799, loss: 0.5266861915588379\n",
            "Checkpoint is saved\n",
            "step: 5804, loss: 0.3528409004211426\n",
            "step: 5809, loss: 0.347337543964386\n",
            "step: 5814, loss: 0.46174854040145874\n",
            "step: 5819, loss: 0.4171554446220398\n",
            "Checkpoint is saved\n",
            "step: 5824, loss: 0.4625520706176758\n",
            "step: 5829, loss: 0.48407649993896484\n",
            "step: 5834, loss: 0.4069470763206482\n",
            "step: 5839, loss: 0.40865394473075867\n",
            "Checkpoint is saved\n",
            "step: 5844, loss: 0.4275210499763489\n",
            "step: 5849, loss: 0.39621710777282715\n",
            "step: 5854, loss: 0.5256966352462769\n",
            "step: 5859, loss: 0.34086376428604126\n",
            "Checkpoint is saved\n",
            "step: 5864, loss: 0.2496013194322586\n",
            "step: 5869, loss: 0.3604961335659027\n",
            "step: 5874, loss: 0.5630531907081604\n",
            "step: 5879, loss: 0.5226340293884277\n",
            "Checkpoint is saved\n",
            "step: 5884, loss: 0.42373859882354736\n",
            "step: 5889, loss: 0.37769514322280884\n",
            "step: 5894, loss: 0.36602649092674255\n",
            "step: 5899, loss: 0.4037967324256897\n",
            "Checkpoint is saved\n",
            "step: 5904, loss: 0.4628312587738037\n",
            "step: 5909, loss: 0.38529443740844727\n",
            "step: 5914, loss: 0.41758042573928833\n",
            "step: 5919, loss: 0.42144137620925903\n",
            "Checkpoint is saved\n",
            "step: 5924, loss: 0.3553449809551239\n",
            "step: 5929, loss: 0.48124709725379944\n",
            "step: 5934, loss: 0.44970667362213135\n",
            "step: 5939, loss: 0.38283511996269226\n",
            "Checkpoint is saved\n",
            "step: 5944, loss: 0.36629801988601685\n",
            "step: 5949, loss: 0.3626634180545807\n",
            "step: 5954, loss: 0.3014335334300995\n",
            "step: 5959, loss: 0.40022146701812744\n",
            "Checkpoint is saved\n",
            "step: 5964, loss: 0.49336886405944824\n",
            "step: 5969, loss: 0.46081265807151794\n",
            "step: 5974, loss: 0.6047179102897644\n",
            "step: 5979, loss: 0.4116130471229553\n",
            "Checkpoint is saved\n",
            "step: 5984, loss: 0.35326117277145386\n",
            "step: 5989, loss: 0.37328672409057617\n",
            "step: 5994, loss: 0.38628023862838745\n",
            "step: 5999, loss: 0.5007874369621277\n",
            "Checkpoint is saved\n",
            "step: 6004, loss: 0.4882912039756775\n",
            "step: 6009, loss: 0.3245001435279846\n",
            "step: 6014, loss: 0.4214766025543213\n",
            "step: 6019, loss: 0.40630224347114563\n",
            "Checkpoint is saved\n",
            "step: 6024, loss: 0.4279007613658905\n",
            "step: 6029, loss: 0.3731440305709839\n",
            "step: 6034, loss: 0.4681365489959717\n",
            "step: 6039, loss: 0.4214518666267395\n",
            "Checkpoint is saved\n",
            "step: 6044, loss: 0.31939566135406494\n",
            "step: 6049, loss: 0.4653811454772949\n",
            "step: 6054, loss: 0.4467030465602875\n",
            "step: 6059, loss: 0.2944965064525604\n",
            "Checkpoint is saved\n",
            "step: 6064, loss: 0.4209565818309784\n",
            "step: 6069, loss: 0.39883357286453247\n",
            "step: 6074, loss: 0.3582074046134949\n",
            "step: 6079, loss: 0.3951571583747864\n",
            "Checkpoint is saved\n",
            "step: 6084, loss: 0.3951031565666199\n",
            "step: 6089, loss: 0.3649352192878723\n",
            "step: 6094, loss: 0.34055882692337036\n",
            "step: 6099, loss: 0.3807154893875122\n",
            "Checkpoint is saved\n",
            "step: 6104, loss: 0.430641770362854\n",
            "step: 6109, loss: 0.42144080996513367\n",
            "step: 6114, loss: 0.3427116870880127\n",
            "step: 6119, loss: 0.33398744463920593\n",
            "Checkpoint is saved\n",
            "step: 6124, loss: 0.37055477499961853\n",
            "step: 6129, loss: 0.2612398564815521\n",
            "step: 6134, loss: 0.5211571455001831\n",
            "step: 6139, loss: 0.3616625666618347\n",
            "Checkpoint is saved\n",
            "step: 6144, loss: 0.37264543771743774\n",
            "step: 6149, loss: 0.420967161655426\n",
            "step: 6154, loss: 0.40663161873817444\n",
            "step: 6159, loss: 0.37225717306137085\n",
            "Checkpoint is saved\n",
            "step: 6164, loss: 0.4124165177345276\n",
            "step: 6169, loss: 0.3546205163002014\n",
            "step: 6174, loss: 0.5443280935287476\n",
            "step: 6179, loss: 0.40166229009628296\n",
            "Checkpoint is saved\n",
            "step: 6184, loss: 0.4940258860588074\n",
            "step: 6189, loss: 0.4093683362007141\n",
            "step: 6194, loss: 0.4736422002315521\n",
            "step: 6199, loss: 0.4143899083137512\n",
            "Checkpoint is saved\n",
            "step: 6204, loss: 0.34503307938575745\n",
            "step: 6209, loss: 0.38604140281677246\n",
            "step: 6214, loss: 0.5593307614326477\n",
            "step: 6219, loss: 0.5210855007171631\n",
            "Checkpoint is saved\n",
            "step: 6224, loss: 0.39071714878082275\n",
            "step: 6229, loss: 0.35976579785346985\n",
            "step: 6234, loss: 0.3430019021034241\n",
            "step: 6239, loss: 0.3214428126811981\n",
            "Checkpoint is saved\n",
            "step: 6244, loss: 0.48025721311569214\n",
            "step: 6249, loss: 0.3743029236793518\n",
            "step: 6254, loss: 0.3826579749584198\n",
            "step: 6259, loss: 0.4646710455417633\n",
            "Checkpoint is saved\n",
            "step: 6264, loss: 0.4426611363887787\n",
            "step: 6269, loss: 0.4213182032108307\n",
            "step: 6274, loss: 0.4564743936061859\n",
            "step: 6279, loss: 0.4395124018192291\n",
            "Checkpoint is saved\n",
            "step: 6284, loss: 0.4141883850097656\n",
            "step: 6289, loss: 0.5254600048065186\n",
            "step: 6294, loss: 0.4463036060333252\n",
            "step: 6299, loss: 0.47968295216560364\n",
            "Checkpoint is saved\n",
            "step: 6304, loss: 0.3826402723789215\n",
            "step: 6309, loss: 0.4666050970554352\n",
            "step: 6314, loss: 0.40150681138038635\n",
            "step: 6319, loss: 0.42315298318862915\n",
            "Checkpoint is saved\n",
            "step: 6324, loss: 0.5561618804931641\n",
            "step: 6329, loss: 0.24472485482692719\n",
            "step: 6334, loss: 0.4190577268600464\n",
            "step: 6339, loss: 0.42783933877944946\n",
            "Checkpoint is saved\n",
            "step: 6344, loss: 0.3471788763999939\n",
            "step: 6349, loss: 0.3540394902229309\n",
            "step: 6354, loss: 0.4235839247703552\n",
            "step: 6359, loss: 0.5189722776412964\n",
            "Checkpoint is saved\n",
            "step: 6364, loss: 0.3905367851257324\n",
            "step: 6369, loss: 0.4370335042476654\n",
            "step: 6374, loss: 0.30991655588150024\n",
            "step: 6379, loss: 0.41537725925445557\n",
            "Checkpoint is saved\n",
            "step: 6384, loss: 0.4161829352378845\n",
            "step: 6389, loss: 0.5974125266075134\n",
            "step: 6394, loss: 0.4865405857563019\n",
            "step: 6399, loss: 0.37734121084213257\n",
            "Checkpoint is saved\n",
            "step: 6404, loss: 0.3265867531299591\n",
            "step: 6409, loss: 0.33530914783477783\n",
            "step: 6414, loss: 0.39669889211654663\n",
            "step: 6419, loss: 0.3761422634124756\n",
            "Checkpoint is saved\n",
            "step: 6424, loss: 0.44560497999191284\n",
            "step: 6429, loss: 0.3900890350341797\n",
            "step: 6434, loss: 0.40326571464538574\n",
            "step: 6439, loss: 0.4302443265914917\n",
            "Checkpoint is saved\n",
            "step: 6444, loss: 0.4057841897010803\n",
            "step: 6449, loss: 0.2857927083969116\n",
            "step: 6454, loss: 0.4072203040122986\n",
            "step: 6459, loss: 0.38972610235214233\n",
            "Checkpoint is saved\n",
            "step: 6464, loss: 0.39319750666618347\n",
            "step: 6469, loss: 0.40378499031066895\n",
            "step: 6474, loss: 0.45671191811561584\n",
            "step: 6479, loss: 0.3245525062084198\n",
            "Checkpoint is saved\n",
            "step: 6484, loss: 0.4034871459007263\n",
            "step: 6489, loss: 0.5263581275939941\n",
            "step: 6494, loss: 0.3072962760925293\n",
            "step: 6499, loss: 0.4019777178764343\n",
            "Checkpoint is saved\n",
            "step: 6504, loss: 0.4848049581050873\n",
            "step: 6509, loss: 0.38535162806510925\n",
            "step: 6514, loss: 0.4312705397605896\n",
            "step: 6519, loss: 0.3227439820766449\n",
            "Checkpoint is saved\n",
            "step: 6524, loss: 0.3447940945625305\n",
            "step: 6529, loss: 0.45003581047058105\n",
            "step: 6534, loss: 0.3590170443058014\n",
            "step: 6539, loss: 0.42580506205558777\n",
            "Checkpoint is saved\n",
            "step: 6544, loss: 0.3958870470523834\n",
            "step: 6549, loss: 0.4361875057220459\n",
            "step: 6554, loss: 0.391754686832428\n",
            "step: 6559, loss: 0.3163529634475708\n",
            "Checkpoint is saved\n",
            "step: 6564, loss: 0.44854995608329773\n",
            "step: 6569, loss: 0.38426119089126587\n",
            "step: 6574, loss: 0.31824052333831787\n",
            "step: 6579, loss: 0.4177897572517395\n",
            "Checkpoint is saved\n",
            "step: 6584, loss: 0.31822705268859863\n",
            "step: 6589, loss: 0.43745794892311096\n",
            "step: 6594, loss: 0.39138978719711304\n",
            "step: 6599, loss: 0.3075046241283417\n",
            "Checkpoint is saved\n",
            "step: 6604, loss: 0.3526152968406677\n",
            "step: 6609, loss: 0.3931310772895813\n",
            "step: 6614, loss: 0.36697685718536377\n",
            "step: 6619, loss: 0.41039344668388367\n",
            "Checkpoint is saved\n",
            "step: 6624, loss: 0.46273940801620483\n",
            "step: 6629, loss: 0.35211244225502014\n",
            "step: 6634, loss: 0.46764010190963745\n",
            "step: 6639, loss: 0.4870501160621643\n",
            "Checkpoint is saved\n",
            "step: 6644, loss: 0.48089879751205444\n",
            "step: 6649, loss: 0.6010978817939758\n",
            "step: 6654, loss: 0.3756198585033417\n",
            "step: 6659, loss: 0.35678020119667053\n",
            "Checkpoint is saved\n",
            "step: 6664, loss: 0.31516897678375244\n",
            "step: 6669, loss: 0.44527527689933777\n",
            "step: 6674, loss: 0.3009268641471863\n",
            "step: 6679, loss: 0.48787999153137207\n",
            "Checkpoint is saved\n",
            "step: 6684, loss: 0.3259497284889221\n",
            "step: 6689, loss: 0.4331202208995819\n",
            "step: 6694, loss: 0.4414203464984894\n",
            "step: 6699, loss: 0.3440142571926117\n",
            "Checkpoint is saved\n",
            "step: 6704, loss: 0.36168861389160156\n",
            "step: 6709, loss: 0.28573060035705566\n",
            "step: 6714, loss: 0.3699987530708313\n",
            "step: 6719, loss: 0.518989086151123\n",
            "Checkpoint is saved\n",
            "step: 6724, loss: 0.4023253917694092\n",
            "step: 6729, loss: 0.36053216457366943\n",
            "step: 6734, loss: 0.3859221041202545\n",
            "step: 6739, loss: 0.3211718201637268\n",
            "Checkpoint is saved\n",
            "step: 6744, loss: 0.3702118396759033\n",
            "step: 6749, loss: 0.46991708874702454\n",
            "step: 6754, loss: 0.3755943179130554\n",
            "step: 6759, loss: 0.43383553624153137\n",
            "Checkpoint is saved\n",
            "step: 6764, loss: 0.40728551149368286\n",
            "step: 6769, loss: 0.34901174902915955\n",
            "step: 6774, loss: 0.41288942098617554\n",
            "step: 6779, loss: 0.3328457474708557\n",
            "Checkpoint is saved\n",
            "step: 6784, loss: 0.37122035026550293\n",
            "step: 6789, loss: 0.4185045063495636\n",
            "step: 6794, loss: 0.3271768093109131\n",
            "step: 6799, loss: 0.4076639413833618\n",
            "Checkpoint is saved\n",
            "step: 6804, loss: 0.37350356578826904\n",
            "step: 6809, loss: 0.4703381359577179\n",
            "step: 6814, loss: 0.4231231212615967\n",
            "step: 6819, loss: 0.5358541011810303\n",
            "Checkpoint is saved\n",
            "step: 6824, loss: 0.35323017835617065\n",
            "step: 6829, loss: 0.3442171812057495\n",
            "step: 6834, loss: 0.4294882118701935\n",
            "step: 6839, loss: 0.410123348236084\n",
            "Checkpoint is saved\n",
            "step: 6844, loss: 0.3188285827636719\n",
            "step: 6849, loss: 0.3822984993457794\n",
            "step: 6854, loss: 0.29848605394363403\n",
            "step: 6859, loss: 0.37867939472198486\n",
            "Checkpoint is saved\n",
            "step: 6864, loss: 0.43767666816711426\n",
            "step: 6869, loss: 0.427779883146286\n",
            "step: 6874, loss: 0.3058493137359619\n",
            "step: 6879, loss: 0.35386162996292114\n",
            "Checkpoint is saved\n",
            "step: 6884, loss: 0.47202813625335693\n",
            "step: 6889, loss: 0.501427412033081\n",
            "step: 6894, loss: 0.5910796523094177\n",
            "step: 6899, loss: 0.35230135917663574\n",
            "Checkpoint is saved\n",
            "step: 6904, loss: 0.36669349670410156\n",
            "step: 6909, loss: 0.43882930278778076\n",
            "step: 6914, loss: 0.35551580786705017\n",
            "step: 6919, loss: 0.35872405767440796\n",
            "Checkpoint is saved\n",
            "step: 6924, loss: 0.36384642124176025\n",
            "step: 6929, loss: 0.4738593101501465\n",
            "step: 6934, loss: 0.3463384211063385\n",
            "step: 6939, loss: 0.4166663885116577\n",
            "Checkpoint is saved\n",
            "step: 6944, loss: 0.43188780546188354\n",
            "step: 6949, loss: 0.42788252234458923\n",
            "step: 6954, loss: 0.33485177159309387\n",
            "step: 6959, loss: 0.3420791029930115\n",
            "Checkpoint is saved\n",
            "step: 6964, loss: 0.3259599804878235\n",
            "step: 6969, loss: 0.3538898825645447\n",
            "step: 6974, loss: 0.35074251890182495\n",
            "step: 6979, loss: 0.4486239552497864\n",
            "Checkpoint is saved\n",
            "step: 6984, loss: 0.3509858548641205\n",
            "step: 6989, loss: 0.4200598895549774\n",
            "step: 6994, loss: 0.44173699617385864\n",
            "step: 6999, loss: 0.41774749755859375\n",
            "Checkpoint is saved\n",
            "step: 7004, loss: 0.380182147026062\n",
            "step: 7009, loss: 0.5174466371536255\n",
            "step: 7014, loss: 0.45980626344680786\n",
            "step: 7019, loss: 0.4447386562824249\n",
            "Checkpoint is saved\n",
            "step: 7024, loss: 0.3309854567050934\n",
            "step: 7029, loss: 0.3591088354587555\n",
            "step: 7034, loss: 0.2743793725967407\n",
            "step: 7039, loss: 0.3045365512371063\n",
            "Checkpoint is saved\n",
            "step: 7044, loss: 0.3987376093864441\n",
            "step: 7049, loss: 0.4478181004524231\n",
            "step: 7054, loss: 0.35307756066322327\n",
            "step: 7059, loss: 0.40031182765960693\n",
            "Checkpoint is saved\n",
            "step: 7064, loss: 0.3269149959087372\n",
            "step: 7069, loss: 0.39666077494621277\n",
            "step: 7074, loss: 0.33104655146598816\n",
            "step: 7079, loss: 0.44383347034454346\n",
            "Checkpoint is saved\n",
            "step: 7084, loss: 0.29552775621414185\n",
            "step: 7089, loss: 0.2957220673561096\n",
            "step: 7094, loss: 0.37770095467567444\n",
            "step: 7099, loss: 0.31335046887397766\n",
            "Checkpoint is saved\n",
            "step: 7104, loss: 0.45761367678642273\n",
            "step: 7109, loss: 0.4220476746559143\n",
            "step: 7114, loss: 0.3875550925731659\n",
            "step: 7119, loss: 0.35177427530288696\n",
            "Checkpoint is saved\n",
            "step: 7124, loss: 0.3445656895637512\n",
            "step: 7129, loss: 0.46851128339767456\n",
            "step: 7134, loss: 0.4516526460647583\n",
            "step: 7139, loss: 0.361689031124115\n",
            "Checkpoint is saved\n",
            "step: 7144, loss: 0.41805166006088257\n",
            "step: 7149, loss: 0.3543396592140198\n",
            "step: 7154, loss: 0.5047509074211121\n",
            "step: 7159, loss: 0.3433122932910919\n",
            "Checkpoint is saved\n",
            "step: 7164, loss: 0.5010635852813721\n",
            "step: 7169, loss: 0.39994433522224426\n",
            "step: 7174, loss: 0.3884490728378296\n",
            "step: 7179, loss: 0.3224588632583618\n",
            "Checkpoint is saved\n",
            "step: 7184, loss: 0.40792858600616455\n",
            "step: 7189, loss: 0.41040313243865967\n",
            "step: 7194, loss: 0.33933913707733154\n",
            "step: 7199, loss: 0.39017409086227417\n",
            "Checkpoint is saved\n",
            "step: 7204, loss: 0.4299209415912628\n",
            "step: 7209, loss: 0.4190744161605835\n",
            "step: 7214, loss: 0.3465227484703064\n",
            "step: 7219, loss: 0.25598499178886414\n",
            "Checkpoint is saved\n",
            "step: 7224, loss: 0.38045814633369446\n",
            "step: 7229, loss: 0.4289090037345886\n",
            "step: 7234, loss: 0.337396502494812\n",
            "step: 7239, loss: 0.3388306498527527\n",
            "Checkpoint is saved\n",
            "step: 7244, loss: 0.3037344813346863\n",
            "step: 7249, loss: 0.4200347661972046\n",
            "step: 7254, loss: 0.38114306330680847\n",
            "step: 7259, loss: 0.452626496553421\n",
            "Checkpoint is saved\n",
            "step: 7264, loss: 0.3793906569480896\n",
            "step: 7269, loss: 0.4028472602367401\n",
            "step: 7274, loss: 0.36688828468322754\n",
            "step: 7279, loss: 0.28525447845458984\n",
            "Checkpoint is saved\n",
            "step: 7284, loss: 0.41878604888916016\n",
            "step: 7289, loss: 0.3470178246498108\n",
            "step: 7294, loss: 0.36392343044281006\n",
            "step: 7299, loss: 0.2974889874458313\n",
            "Checkpoint is saved\n",
            "step: 7304, loss: 0.3178386688232422\n",
            "step: 7309, loss: 0.2832545340061188\n",
            "step: 7314, loss: 0.4213921129703522\n",
            "step: 7319, loss: 0.4068947434425354\n",
            "Checkpoint is saved\n",
            "step: 7324, loss: 0.47207504510879517\n",
            "step: 7329, loss: 0.3319600224494934\n",
            "step: 7334, loss: 0.37421441078186035\n",
            "step: 7339, loss: 0.40901869535446167\n",
            "Checkpoint is saved\n",
            "step: 7344, loss: 0.3675272464752197\n",
            "step: 7349, loss: 0.3433634638786316\n",
            "step: 7354, loss: 0.36887359619140625\n",
            "step: 7359, loss: 0.38456350564956665\n",
            "Checkpoint is saved\n",
            "step: 7364, loss: 0.41632935404777527\n",
            "step: 7369, loss: 0.32312822341918945\n",
            "step: 7374, loss: 0.2524698078632355\n",
            "step: 7379, loss: 0.399850070476532\n",
            "Checkpoint is saved\n",
            "step: 7384, loss: 0.4272650182247162\n",
            "step: 7389, loss: 0.44370076060295105\n",
            "step: 7394, loss: 0.4631732404232025\n",
            "step: 7399, loss: 0.3877864480018616\n",
            "Checkpoint is saved\n",
            "step: 7404, loss: 0.43710124492645264\n",
            "step: 7409, loss: 0.47813695669174194\n",
            "step: 7414, loss: 0.3223308026790619\n",
            "step: 7419, loss: 0.4747191369533539\n",
            "Checkpoint is saved\n",
            "step: 7424, loss: 0.3691210448741913\n",
            "step: 7429, loss: 0.2483726441860199\n",
            "step: 7434, loss: 0.37425583600997925\n",
            "step: 7439, loss: 0.45356738567352295\n",
            "Checkpoint is saved\n",
            "step: 7444, loss: 0.34801509976387024\n",
            "step: 7449, loss: 0.27111196517944336\n",
            "step: 7454, loss: 0.38771671056747437\n",
            "step: 7459, loss: 0.3389362394809723\n",
            "Checkpoint is saved\n",
            "step: 7464, loss: 0.3933524489402771\n",
            "step: 7469, loss: 0.3960639238357544\n",
            "step: 7474, loss: 0.32101356983184814\n",
            "step: 7479, loss: 0.36417317390441895\n",
            "Checkpoint is saved\n",
            "step: 7484, loss: 0.3671305477619171\n",
            "step: 7489, loss: 0.6152136325836182\n",
            "step: 7494, loss: 0.4139527380466461\n",
            "step: 7499, loss: 0.4664079248905182\n",
            "Checkpoint is saved\n",
            "step: 7504, loss: 0.38922014832496643\n",
            "step: 7509, loss: 0.33521339297294617\n",
            "step: 7514, loss: 0.33870837092399597\n",
            "step: 7519, loss: 0.2945173382759094\n",
            "Checkpoint is saved\n",
            "step: 7524, loss: 0.3407570719718933\n",
            "step: 7529, loss: 0.4949718117713928\n",
            "step: 7534, loss: 0.3168661594390869\n",
            "step: 7539, loss: 0.36149969696998596\n",
            "Checkpoint is saved\n",
            "step: 7544, loss: 0.4148377776145935\n",
            "step: 7549, loss: 0.5014542937278748\n",
            "step: 7554, loss: 0.28618866205215454\n",
            "step: 7559, loss: 0.47448086738586426\n",
            "Checkpoint is saved\n",
            "step: 7564, loss: 0.3665420711040497\n",
            "step: 7569, loss: 0.4021323323249817\n",
            "step: 7574, loss: 0.46274638175964355\n",
            "step: 7579, loss: 0.3537469506263733\n",
            "Checkpoint is saved\n",
            "step: 7584, loss: 0.35989072918891907\n",
            "step: 7589, loss: 0.34891068935394287\n",
            "step: 7594, loss: 0.3715267479419708\n",
            "step: 7599, loss: 0.46231168508529663\n",
            "Checkpoint is saved\n",
            "step: 7604, loss: 0.4078409671783447\n",
            "step: 7609, loss: 0.3224683403968811\n",
            "step: 7614, loss: 0.3812935948371887\n",
            "step: 7619, loss: 0.31801530718803406\n",
            "Checkpoint is saved\n",
            "step: 7624, loss: 0.40641263127326965\n",
            "step: 7629, loss: 0.5144757032394409\n",
            "step: 7634, loss: 0.46270811557769775\n",
            "step: 7639, loss: 0.4250586926937103\n",
            "Checkpoint is saved\n",
            "step: 7644, loss: 0.4641002416610718\n",
            "step: 7649, loss: 0.3922319710254669\n",
            "step: 7654, loss: 0.3339294493198395\n",
            "step: 7659, loss: 0.28910788893699646\n",
            "Checkpoint is saved\n",
            "step: 7664, loss: 0.4682598114013672\n",
            "step: 7669, loss: 0.44872426986694336\n",
            "step: 7674, loss: 0.34427785873413086\n",
            "step: 7679, loss: 0.3953685164451599\n",
            "Checkpoint is saved\n",
            "step: 7684, loss: 0.4320949614048004\n",
            "step: 7689, loss: 0.4153931438922882\n",
            "step: 7694, loss: 0.21766455471515656\n",
            "step: 7699, loss: 0.26505473256111145\n",
            "Checkpoint is saved\n",
            "step: 7704, loss: 0.3409518003463745\n",
            "step: 7709, loss: 0.38453078269958496\n",
            "step: 7714, loss: 0.3519076704978943\n",
            "step: 7719, loss: 0.3474416136741638\n",
            "Checkpoint is saved\n",
            "step: 7724, loss: 0.37536779046058655\n",
            "step: 7729, loss: 0.434942364692688\n",
            "step: 7734, loss: 0.34678947925567627\n",
            "step: 7739, loss: 0.3609834909439087\n",
            "Checkpoint is saved\n",
            "step: 7744, loss: 0.3208068907260895\n",
            "step: 7749, loss: 0.303101122379303\n",
            "step: 7754, loss: 0.3859017491340637\n",
            "step: 7759, loss: 0.3678130507469177\n",
            "Checkpoint is saved\n",
            "step: 7764, loss: 0.3308603763580322\n",
            "step: 7769, loss: 0.48103025555610657\n",
            "step: 7774, loss: 0.34623587131500244\n",
            "step: 7779, loss: 0.2936549186706543\n",
            "Checkpoint is saved\n",
            "step: 7784, loss: 0.445374459028244\n",
            "step: 7789, loss: 0.43496912717819214\n",
            "step: 7794, loss: 0.38626620173454285\n",
            "step: 7799, loss: 0.3201363682746887\n",
            "Checkpoint is saved\n",
            "step: 7804, loss: 0.41905272006988525\n",
            "step: 7809, loss: 0.4094376564025879\n",
            "step: 7814, loss: 0.4379447102546692\n",
            "step: 7819, loss: 0.34897831082344055\n",
            "Checkpoint is saved\n",
            "step: 7824, loss: 0.25873374938964844\n",
            "step: 7829, loss: 0.25508496165275574\n",
            "step: 7834, loss: 0.42945700883865356\n",
            "step: 7839, loss: 0.2745511531829834\n",
            "Checkpoint is saved\n",
            "step: 7844, loss: 0.40113186836242676\n",
            "step: 7849, loss: 0.4795359969139099\n",
            "step: 7854, loss: 0.339597225189209\n",
            "step: 7859, loss: 0.359065443277359\n",
            "Checkpoint is saved\n",
            "step: 7864, loss: 0.4253928065299988\n",
            "step: 7869, loss: 0.31347841024398804\n",
            "step: 7874, loss: 0.5114012956619263\n",
            "step: 7879, loss: 0.49205172061920166\n",
            "Checkpoint is saved\n",
            "step: 7884, loss: 0.3646559417247772\n",
            "step: 7889, loss: 0.38062793016433716\n",
            "step: 7894, loss: 0.32223156094551086\n",
            "step: 7899, loss: 0.40054553747177124\n",
            "Checkpoint is saved\n",
            "step: 7904, loss: 0.35143378376960754\n",
            "step: 7909, loss: 0.26951420307159424\n",
            "step: 7914, loss: 0.2597494125366211\n",
            "step: 7919, loss: 0.3435072898864746\n",
            "Checkpoint is saved\n",
            "step: 7924, loss: 0.261888325214386\n",
            "step: 7929, loss: 0.2860678732395172\n",
            "step: 7934, loss: 0.3805069327354431\n",
            "step: 7939, loss: 0.34501516819000244\n",
            "Checkpoint is saved\n",
            "step: 7944, loss: 0.3228779137134552\n",
            "step: 7949, loss: 0.34591174125671387\n",
            "step: 7954, loss: 0.3095829486846924\n",
            "step: 7959, loss: 0.40024417638778687\n",
            "Checkpoint is saved\n",
            "step: 7964, loss: 0.409598171710968\n",
            "step: 7969, loss: 0.37340670824050903\n",
            "step: 7974, loss: 0.39530783891677856\n",
            "step: 7979, loss: 0.3758547306060791\n",
            "Checkpoint is saved\n",
            "step: 7984, loss: 0.3660411238670349\n",
            "step: 7989, loss: 0.3643338680267334\n",
            "step: 7994, loss: 0.38183343410491943\n",
            "step: 7999, loss: 0.4499632716178894\n",
            "Checkpoint is saved\n",
            "step: 8004, loss: 0.3735477924346924\n",
            "step: 8009, loss: 0.38561826944351196\n",
            "step: 8014, loss: 0.39937824010849\n",
            "step: 8019, loss: 0.4356819987297058\n",
            "Checkpoint is saved\n",
            "step: 8024, loss: 0.3162132203578949\n",
            "step: 8029, loss: 0.37425854802131653\n",
            "step: 8034, loss: 0.45354771614074707\n",
            "step: 8039, loss: 0.40923741459846497\n",
            "Checkpoint is saved\n",
            "step: 8044, loss: 0.35166195034980774\n",
            "step: 8049, loss: 0.37655678391456604\n",
            "step: 8054, loss: 0.3752780258655548\n",
            "step: 8059, loss: 0.48199766874313354\n",
            "Checkpoint is saved\n",
            "step: 8064, loss: 0.33976608514785767\n",
            "step: 8069, loss: 0.4830683469772339\n",
            "step: 8074, loss: 0.4495883285999298\n",
            "step: 8079, loss: 0.4139419198036194\n",
            "Checkpoint is saved\n",
            "step: 8084, loss: 0.4145846962928772\n",
            "step: 8089, loss: 0.31316882371902466\n",
            "step: 8094, loss: 0.37361830472946167\n",
            "step: 8099, loss: 0.30003637075424194\n",
            "Checkpoint is saved\n",
            "step: 8104, loss: 0.4259922504425049\n",
            "step: 8109, loss: 0.30719393491744995\n",
            "step: 8114, loss: 0.29238998889923096\n",
            "step: 8119, loss: 0.48183462023735046\n",
            "Checkpoint is saved\n",
            "step: 8124, loss: 0.46246808767318726\n",
            "step: 8129, loss: 0.3548619747161865\n",
            "step: 8134, loss: 0.2998865246772766\n",
            "step: 8139, loss: 0.5050210952758789\n",
            "Checkpoint is saved\n",
            "step: 8144, loss: 0.30872225761413574\n",
            "step: 8149, loss: 0.30526819825172424\n",
            "step: 8154, loss: 0.35605311393737793\n",
            "step: 8159, loss: 0.4028610587120056\n",
            "Checkpoint is saved\n",
            "step: 8164, loss: 0.3519614040851593\n",
            "step: 8169, loss: 0.2363157570362091\n",
            "step: 8174, loss: 0.33857041597366333\n",
            "step: 8179, loss: 0.33318617939949036\n",
            "Checkpoint is saved\n",
            "step: 8184, loss: 0.35110583901405334\n",
            "step: 8189, loss: 0.34154176712036133\n",
            "step: 8194, loss: 0.3604884743690491\n",
            "step: 8199, loss: 0.3180415630340576\n",
            "Checkpoint is saved\n",
            "step: 8204, loss: 0.39088383316993713\n",
            "step: 8209, loss: 0.3619322180747986\n",
            "step: 8214, loss: 0.4069640040397644\n",
            "step: 8219, loss: 0.3358209729194641\n",
            "Checkpoint is saved\n",
            "step: 8224, loss: 0.3403473198413849\n",
            "step: 8229, loss: 0.31691598892211914\n",
            "step: 8234, loss: 0.3506321609020233\n",
            "step: 8239, loss: 0.2852897644042969\n",
            "Checkpoint is saved\n",
            "step: 8244, loss: 0.34950190782546997\n",
            "step: 8249, loss: 0.4374273121356964\n",
            "step: 8254, loss: 0.3272554278373718\n",
            "step: 8259, loss: 0.3404567539691925\n",
            "Checkpoint is saved\n",
            "step: 8264, loss: 0.3749646544456482\n",
            "step: 8269, loss: 0.34854474663734436\n",
            "step: 8274, loss: 0.31289607286453247\n",
            "step: 8279, loss: 0.33656996488571167\n",
            "Checkpoint is saved\n",
            "step: 8284, loss: 0.30084365606307983\n",
            "step: 8289, loss: 0.44419458508491516\n",
            "step: 8294, loss: 0.2899211049079895\n",
            "step: 8299, loss: 0.3540264964103699\n",
            "Checkpoint is saved\n",
            "step: 8304, loss: 0.37195223569869995\n",
            "step: 8309, loss: 0.3138286769390106\n",
            "step: 8314, loss: 0.2521675229072571\n",
            "step: 8319, loss: 0.260664165019989\n",
            "Checkpoint is saved\n",
            "step: 8324, loss: 0.3517961800098419\n",
            "step: 8329, loss: 0.3270205557346344\n",
            "step: 8334, loss: 0.3921451270580292\n",
            "step: 8339, loss: 0.3459598124027252\n",
            "Checkpoint is saved\n",
            "step: 8344, loss: 0.3876267671585083\n",
            "step: 8349, loss: 0.44412946701049805\n",
            "step: 8354, loss: 0.396928071975708\n",
            "step: 8359, loss: 0.4093301296234131\n",
            "Checkpoint is saved\n",
            "step: 8364, loss: 0.3041553497314453\n",
            "step: 8369, loss: 0.4204184412956238\n",
            "step: 8374, loss: 0.38758939504623413\n",
            "step: 8379, loss: 0.3253002166748047\n",
            "Checkpoint is saved\n",
            "step: 8384, loss: 0.4139899015426636\n",
            "step: 8389, loss: 0.2889552712440491\n",
            "step: 8394, loss: 0.38982266187667847\n",
            "step: 8399, loss: 0.4158795475959778\n",
            "Checkpoint is saved\n",
            "step: 8404, loss: 0.3261791467666626\n",
            "step: 8409, loss: 0.41836225986480713\n",
            "step: 8414, loss: 0.41511809825897217\n",
            "step: 8419, loss: 0.35432201623916626\n",
            "Checkpoint is saved\n",
            "step: 8424, loss: 0.30167871713638306\n",
            "step: 8429, loss: 0.34666377305984497\n",
            "step: 8434, loss: 0.3893768787384033\n",
            "step: 8439, loss: 0.3247937262058258\n",
            "Checkpoint is saved\n",
            "step: 8444, loss: 0.43107378482818604\n",
            "step: 8449, loss: 0.26857832074165344\n",
            "step: 8454, loss: 0.39526399970054626\n",
            "step: 8459, loss: 0.36266854405403137\n",
            "Checkpoint is saved\n",
            "step: 8464, loss: 0.3493066132068634\n",
            "step: 8469, loss: 0.43605509400367737\n",
            "step: 8474, loss: 0.30401116609573364\n",
            "step: 8479, loss: 0.40450939536094666\n",
            "Checkpoint is saved\n",
            "step: 8484, loss: 0.3537254333496094\n",
            "step: 8489, loss: 0.4284525215625763\n",
            "step: 8494, loss: 0.33788567781448364\n",
            "step: 8499, loss: 0.33117878437042236\n",
            "Checkpoint is saved\n",
            "step: 8504, loss: 0.32449424266815186\n",
            "step: 8509, loss: 0.3789346516132355\n",
            "step: 8514, loss: 0.426189661026001\n",
            "step: 8519, loss: 0.34662139415740967\n",
            "Checkpoint is saved\n",
            "step: 8524, loss: 0.3827771246433258\n",
            "step: 8529, loss: 0.34074026346206665\n",
            "step: 8534, loss: 0.39398670196533203\n",
            "step: 8539, loss: 0.2947472929954529\n",
            "Checkpoint is saved\n",
            "step: 8544, loss: 0.3564090132713318\n",
            "step: 8549, loss: 0.4643508195877075\n",
            "step: 8554, loss: 0.3806495666503906\n",
            "step: 8559, loss: 0.24775655567646027\n",
            "Checkpoint is saved\n",
            "step: 8564, loss: 0.3940902054309845\n",
            "step: 8569, loss: 0.27549028396606445\n",
            "step: 8574, loss: 0.4043959081172943\n",
            "step: 8579, loss: 0.38216865062713623\n",
            "Checkpoint is saved\n",
            "step: 8584, loss: 0.40927600860595703\n",
            "step: 8589, loss: 0.3718741536140442\n",
            "step: 8594, loss: 0.40700989961624146\n",
            "step: 8599, loss: 0.35612723231315613\n",
            "Checkpoint is saved\n",
            "step: 8604, loss: 0.30549710988998413\n",
            "step: 8609, loss: 0.2984124720096588\n",
            "step: 8614, loss: 0.28819704055786133\n",
            "step: 8619, loss: 0.3415730595588684\n",
            "Checkpoint is saved\n",
            "step: 8624, loss: 0.4280976355075836\n",
            "step: 8629, loss: 0.286675363779068\n",
            "step: 8634, loss: 0.2778712511062622\n",
            "step: 8639, loss: 0.3205539584159851\n",
            "Checkpoint is saved\n",
            "step: 8644, loss: 0.26920410990715027\n",
            "step: 8649, loss: 0.3414674401283264\n",
            "step: 8654, loss: 0.4210726022720337\n",
            "step: 8659, loss: 0.4106028079986572\n",
            "Checkpoint is saved\n",
            "step: 8664, loss: 0.23042798042297363\n",
            "step: 8669, loss: 0.33436644077301025\n",
            "step: 8674, loss: 0.3943858742713928\n",
            "step: 8679, loss: 0.3268856406211853\n",
            "Checkpoint is saved\n",
            "step: 8684, loss: 0.3481990098953247\n",
            "step: 8689, loss: 0.4368169903755188\n",
            "step: 8694, loss: 0.45410841703414917\n",
            "step: 8699, loss: 0.3816891014575958\n",
            "Checkpoint is saved\n",
            "step: 8704, loss: 0.4426027536392212\n",
            "step: 8709, loss: 0.3796008825302124\n",
            "step: 8714, loss: 0.46430763602256775\n",
            "step: 8719, loss: 0.4199645519256592\n",
            "Checkpoint is saved\n",
            "step: 8724, loss: 0.29031434655189514\n",
            "step: 8729, loss: 0.29483762383461\n",
            "step: 8734, loss: 0.35351574420928955\n",
            "step: 8739, loss: 0.3906727433204651\n",
            "Checkpoint is saved\n",
            "step: 8744, loss: 0.34014615416526794\n",
            "step: 8749, loss: 0.3868591785430908\n",
            "step: 8754, loss: 0.29035139083862305\n",
            "step: 8759, loss: 0.2793205678462982\n",
            "Checkpoint is saved\n",
            "step: 8764, loss: 0.4401102066040039\n",
            "step: 8769, loss: 0.3125942647457123\n",
            "step: 8774, loss: 0.3330351412296295\n",
            "step: 8779, loss: 0.42638370394706726\n",
            "Checkpoint is saved\n",
            "step: 8784, loss: 0.3812863230705261\n",
            "step: 8789, loss: 0.36171141266822815\n",
            "step: 8794, loss: 0.23234309256076813\n",
            "step: 8799, loss: 0.3129009008407593\n",
            "Checkpoint is saved\n",
            "step: 8804, loss: 0.3113766610622406\n",
            "step: 8809, loss: 0.33288073539733887\n",
            "step: 8814, loss: 0.27034273743629456\n",
            "step: 8819, loss: 0.43049758672714233\n",
            "Checkpoint is saved\n",
            "step: 8824, loss: 0.26491841673851013\n",
            "step: 8829, loss: 0.3407790958881378\n",
            "step: 8834, loss: 0.34396892786026\n",
            "step: 8839, loss: 0.3599247336387634\n",
            "Checkpoint is saved\n",
            "step: 8844, loss: 0.3141743540763855\n",
            "step: 8849, loss: 0.40240389108657837\n",
            "step: 8854, loss: 0.33583807945251465\n",
            "step: 8859, loss: 0.5012214183807373\n",
            "Checkpoint is saved\n",
            "step: 8864, loss: 0.4080039858818054\n",
            "step: 8869, loss: 0.34010908007621765\n",
            "step: 8874, loss: 0.3537260890007019\n",
            "step: 8879, loss: 0.3717625141143799\n",
            "Checkpoint is saved\n",
            "step: 8884, loss: 0.261646032333374\n",
            "step: 8889, loss: 0.40457916259765625\n",
            "step: 8894, loss: 0.3363034725189209\n",
            "step: 8899, loss: 0.26939886808395386\n",
            "Checkpoint is saved\n",
            "step: 8904, loss: 0.3456599712371826\n",
            "step: 8909, loss: 0.32435286045074463\n",
            "step: 8914, loss: 0.3457965850830078\n",
            "step: 8919, loss: 0.29352736473083496\n",
            "Checkpoint is saved\n",
            "step: 8924, loss: 0.3879893124103546\n",
            "step: 8929, loss: 0.3571140170097351\n",
            "step: 8934, loss: 0.3260912299156189\n",
            "step: 8939, loss: 0.36244845390319824\n",
            "Checkpoint is saved\n",
            "step: 8944, loss: 0.39592766761779785\n",
            "step: 8949, loss: 0.3154037892818451\n",
            "step: 8954, loss: 0.30260413885116577\n",
            "step: 8959, loss: 0.3052942156791687\n",
            "Checkpoint is saved\n",
            "step: 8964, loss: 0.3637735843658447\n",
            "step: 8969, loss: 0.3770374059677124\n",
            "step: 8974, loss: 0.31212204694747925\n",
            "step: 8979, loss: 0.4039456844329834\n",
            "Checkpoint is saved\n",
            "step: 8984, loss: 0.26178407669067383\n",
            "step: 8989, loss: 0.3237137198448181\n",
            "step: 8994, loss: 0.2907019555568695\n",
            "step: 8999, loss: 0.3682903051376343\n",
            "Checkpoint is saved\n",
            "step: 9004, loss: 0.35050857067108154\n",
            "step: 9009, loss: 0.24993851780891418\n",
            "step: 9014, loss: 0.4027034044265747\n",
            "step: 9019, loss: 0.31590044498443604\n",
            "Checkpoint is saved\n",
            "step: 9024, loss: 0.33331382274627686\n",
            "step: 9029, loss: 0.3446205258369446\n",
            "step: 9034, loss: 0.4503895342350006\n",
            "step: 9039, loss: 0.3484002947807312\n",
            "Checkpoint is saved\n",
            "step: 9044, loss: 0.40859153866767883\n",
            "step: 9049, loss: 0.294353723526001\n",
            "step: 9054, loss: 0.37029361724853516\n",
            "step: 9059, loss: 0.30700358748435974\n",
            "Checkpoint is saved\n",
            "step: 9064, loss: 0.3995625376701355\n",
            "step: 9069, loss: 0.46462857723236084\n",
            "step: 9074, loss: 0.4027109146118164\n",
            "step: 9079, loss: 0.4531160891056061\n",
            "Checkpoint is saved\n",
            "step: 9084, loss: 0.2964463531970978\n",
            "step: 9089, loss: 0.34033992886543274\n",
            "step: 9094, loss: 0.3913390040397644\n",
            "step: 9099, loss: 0.29789793491363525\n",
            "Checkpoint is saved\n",
            "step: 9104, loss: 0.4538893699645996\n",
            "step: 9109, loss: 0.4211423397064209\n",
            "step: 9114, loss: 0.34218719601631165\n",
            "step: 9119, loss: 0.382851243019104\n",
            "Checkpoint is saved\n",
            "step: 9124, loss: 0.47164082527160645\n",
            "step: 9129, loss: 0.4305775761604309\n",
            "step: 9134, loss: 0.3370428681373596\n",
            "step: 9139, loss: 0.27467966079711914\n",
            "Checkpoint is saved\n",
            "step: 9144, loss: 0.27193599939346313\n",
            "step: 9149, loss: 0.34835004806518555\n",
            "step: 9154, loss: 0.3018004298210144\n",
            "step: 9159, loss: 0.4575449824333191\n",
            "Checkpoint is saved\n",
            "step: 9164, loss: 0.3125320076942444\n",
            "step: 9169, loss: 0.4477739930152893\n",
            "step: 9174, loss: 0.24642322957515717\n",
            "step: 9179, loss: 0.32727906107902527\n",
            "Checkpoint is saved\n",
            "step: 9184, loss: 0.2767331004142761\n",
            "step: 9189, loss: 0.281885027885437\n",
            "step: 9194, loss: 0.34120380878448486\n",
            "step: 9199, loss: 0.36429959535598755\n",
            "Checkpoint is saved\n",
            "step: 9204, loss: 0.29791736602783203\n",
            "step: 9209, loss: 0.4417371153831482\n",
            "step: 9214, loss: 0.34707820415496826\n",
            "step: 9219, loss: 0.35846835374832153\n",
            "Checkpoint is saved\n",
            "step: 9224, loss: 0.31856995820999146\n",
            "step: 9229, loss: 0.33998116850852966\n",
            "step: 9234, loss: 0.36213570833206177\n",
            "step: 9239, loss: 0.40850964188575745\n",
            "Checkpoint is saved\n",
            "step: 9244, loss: 0.3022726774215698\n",
            "step: 9249, loss: 0.34093189239501953\n",
            "step: 9254, loss: 0.36913323402404785\n",
            "step: 9259, loss: 0.32269957661628723\n",
            "Checkpoint is saved\n",
            "step: 9264, loss: 0.3187035620212555\n",
            "step: 9269, loss: 0.38191869854927063\n",
            "step: 9274, loss: 0.35811907052993774\n",
            "step: 9279, loss: 0.31691262125968933\n",
            "Checkpoint is saved\n",
            "step: 9284, loss: 0.2860374450683594\n",
            "step: 9289, loss: 0.38118261098861694\n",
            "step: 9294, loss: 0.4036903381347656\n",
            "step: 9299, loss: 0.36310869455337524\n",
            "Checkpoint is saved\n",
            "step: 9304, loss: 0.3219859004020691\n",
            "step: 9309, loss: 0.34281638264656067\n",
            "step: 9314, loss: 0.32645171880722046\n",
            "step: 9319, loss: 0.4115031063556671\n",
            "Checkpoint is saved\n",
            "step: 9324, loss: 0.4006805419921875\n",
            "step: 9329, loss: 0.3421335220336914\n",
            "step: 9334, loss: 0.30567044019699097\n",
            "step: 9339, loss: 0.3008917570114136\n",
            "Checkpoint is saved\n",
            "step: 9344, loss: 0.3513774275779724\n",
            "step: 9349, loss: 0.38409772515296936\n",
            "step: 9354, loss: 0.30173563957214355\n",
            "step: 9359, loss: 0.3683777153491974\n",
            "Checkpoint is saved\n",
            "step: 9364, loss: 0.400748074054718\n",
            "step: 9369, loss: 0.3423687219619751\n",
            "step: 9374, loss: 0.3021896779537201\n",
            "step: 9379, loss: 0.315588116645813\n",
            "Checkpoint is saved\n",
            "step: 9384, loss: 0.26746881008148193\n",
            "step: 9389, loss: 0.3719496726989746\n",
            "step: 9394, loss: 0.30627626180648804\n",
            "step: 9399, loss: 0.4710931181907654\n",
            "Checkpoint is saved\n",
            "step: 9404, loss: 0.39890632033348083\n",
            "step: 9409, loss: 0.33450424671173096\n",
            "step: 9414, loss: 0.28051871061325073\n",
            "step: 9419, loss: 0.48655545711517334\n",
            "Checkpoint is saved\n",
            "step: 9424, loss: 0.3536350429058075\n",
            "step: 9429, loss: 0.2786294221878052\n",
            "step: 9434, loss: 0.25917860865592957\n",
            "step: 9439, loss: 0.33056968450546265\n",
            "Checkpoint is saved\n",
            "step: 9444, loss: 0.24012619256973267\n",
            "step: 9449, loss: 0.36355751752853394\n",
            "step: 9454, loss: 0.23977956175804138\n",
            "step: 9459, loss: 0.39034557342529297\n",
            "Checkpoint is saved\n",
            "step: 9464, loss: 0.35764679312705994\n",
            "step: 9469, loss: 0.38111621141433716\n",
            "step: 9474, loss: 0.33039331436157227\n",
            "step: 9479, loss: 0.28325119614601135\n",
            "Checkpoint is saved\n",
            "step: 9484, loss: 0.4411822259426117\n",
            "step: 9489, loss: 0.3884817361831665\n",
            "step: 9494, loss: 0.3245536684989929\n",
            "step: 9499, loss: 0.3468616306781769\n",
            "Checkpoint is saved\n",
            "step: 9504, loss: 0.35415786504745483\n",
            "step: 9509, loss: 0.32436448335647583\n",
            "step: 9514, loss: 0.4240776300430298\n",
            "step: 9519, loss: 0.31297552585601807\n",
            "Checkpoint is saved\n",
            "step: 9524, loss: 0.5145825147628784\n",
            "step: 9529, loss: 0.4146803915500641\n",
            "step: 9534, loss: 0.37324512004852295\n",
            "step: 9539, loss: 0.31674784421920776\n",
            "Checkpoint is saved\n",
            "step: 9544, loss: 0.41207265853881836\n",
            "step: 9549, loss: 0.2853832244873047\n",
            "step: 9554, loss: 0.48742562532424927\n",
            "step: 9559, loss: 0.28780028223991394\n",
            "Checkpoint is saved\n",
            "step: 9564, loss: 0.33730795979499817\n",
            "step: 9569, loss: 0.32065266370773315\n",
            "step: 9574, loss: 0.4456510543823242\n",
            "step: 9579, loss: 0.3972489535808563\n",
            "Checkpoint is saved\n",
            "step: 9584, loss: 0.30057859420776367\n",
            "step: 9589, loss: 0.46249669790267944\n",
            "step: 9594, loss: 0.3150123357772827\n",
            "step: 9599, loss: 0.3239392638206482\n",
            "Checkpoint is saved\n",
            "step: 9604, loss: 0.39716675877571106\n",
            "step: 9609, loss: 0.32156604528427124\n",
            "step: 9614, loss: 0.3634124994277954\n",
            "step: 9619, loss: 0.2645612955093384\n",
            "Checkpoint is saved\n",
            "step: 9624, loss: 0.29512161016464233\n",
            "step: 9629, loss: 0.5516083240509033\n",
            "step: 9634, loss: 0.26576507091522217\n",
            "step: 9639, loss: 0.31829833984375\n",
            "Checkpoint is saved\n",
            "step: 9644, loss: 0.45421531796455383\n",
            "step: 9649, loss: 0.37006598711013794\n",
            "step: 9654, loss: 0.37739482522010803\n",
            "step: 9659, loss: 0.3807206153869629\n",
            "Checkpoint is saved\n",
            "step: 9664, loss: 0.30326539278030396\n",
            "step: 9669, loss: 0.3088109493255615\n",
            "step: 9674, loss: 0.3769552707672119\n",
            "step: 9679, loss: 0.2222723364830017\n",
            "Checkpoint is saved\n",
            "step: 9684, loss: 0.27472954988479614\n",
            "step: 9689, loss: 0.32266607880592346\n",
            "step: 9694, loss: 0.3677506744861603\n",
            "step: 9699, loss: 0.4190245568752289\n",
            "Checkpoint is saved\n",
            "step: 9704, loss: 0.3481884300708771\n",
            "step: 9709, loss: 0.3816567659378052\n",
            "step: 9714, loss: 0.36200258135795593\n",
            "step: 9719, loss: 0.31008511781692505\n",
            "Checkpoint is saved\n",
            "step: 9724, loss: 0.3690844178199768\n",
            "step: 9729, loss: 0.28193551301956177\n",
            "step: 9734, loss: 0.22368094325065613\n",
            "step: 9739, loss: 0.43359893560409546\n",
            "Checkpoint is saved\n",
            "step: 9744, loss: 0.20771537721157074\n",
            "step: 9749, loss: 0.32950514554977417\n",
            "step: 9754, loss: 0.37816354632377625\n",
            "step: 9759, loss: 0.33993440866470337\n",
            "Checkpoint is saved\n",
            "step: 9764, loss: 0.3202538788318634\n",
            "step: 9769, loss: 0.365700900554657\n",
            "step: 9774, loss: 0.33072713017463684\n",
            "step: 9779, loss: 0.39061447978019714\n",
            "Checkpoint is saved\n",
            "step: 9784, loss: 0.308748334646225\n",
            "step: 9789, loss: 0.3270392417907715\n",
            "step: 9794, loss: 0.36058223247528076\n",
            "step: 9799, loss: 0.3536512851715088\n",
            "Checkpoint is saved\n",
            "step: 9804, loss: 0.4545508921146393\n",
            "step: 9809, loss: 0.3810936212539673\n",
            "step: 9814, loss: 0.389251708984375\n",
            "step: 9819, loss: 0.36843734979629517\n",
            "Checkpoint is saved\n",
            "step: 9824, loss: 0.4096171259880066\n",
            "step: 9829, loss: 0.2880131006240845\n",
            "step: 9834, loss: 0.3098572790622711\n",
            "step: 9839, loss: 0.35431957244873047\n",
            "Checkpoint is saved\n",
            "step: 9844, loss: 0.3134765625\n",
            "step: 9849, loss: 0.4218382239341736\n",
            "step: 9854, loss: 0.43714115023612976\n",
            "step: 9859, loss: 0.3558753728866577\n",
            "Checkpoint is saved\n",
            "step: 9864, loss: 0.382794052362442\n",
            "step: 9869, loss: 0.3491158187389374\n",
            "step: 9874, loss: 0.38971853256225586\n",
            "step: 9879, loss: 0.36116334795951843\n",
            "Checkpoint is saved\n",
            "step: 9884, loss: 0.25548046827316284\n",
            "step: 9889, loss: 0.2822829484939575\n",
            "step: 9894, loss: 0.33819735050201416\n",
            "step: 9899, loss: 0.30498412251472473\n",
            "Checkpoint is saved\n",
            "step: 9904, loss: 0.40866905450820923\n",
            "step: 9909, loss: 0.41729581356048584\n",
            "step: 9914, loss: 0.3171410858631134\n",
            "step: 9919, loss: 0.2535872757434845\n",
            "Checkpoint is saved\n",
            "step: 9924, loss: 0.36199384927749634\n",
            "step: 9929, loss: 0.245239719748497\n",
            "step: 9934, loss: 0.29661962389945984\n",
            "step: 9939, loss: 0.5209231376647949\n",
            "Checkpoint is saved\n",
            "step: 9944, loss: 0.31080320477485657\n",
            "step: 9949, loss: 0.3664649724960327\n",
            "step: 9954, loss: 0.3873822093009949\n",
            "step: 9959, loss: 0.4717671275138855\n",
            "Checkpoint is saved\n",
            "step: 9964, loss: 0.38135746121406555\n",
            "step: 9969, loss: 0.22591447830200195\n",
            "step: 9974, loss: 0.34182924032211304\n",
            "step: 9979, loss: 0.3671790361404419\n",
            "Checkpoint is saved\n",
            "step: 9984, loss: 0.279151052236557\n",
            "step: 9989, loss: 0.4048398435115814\n",
            "step: 9994, loss: 0.21039822697639465\n",
            "step: 9999, loss: 0.28294700384140015\n",
            "Checkpoint is saved\n",
            "Training time for 10000 steps: 3737.690236568451s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o3zeizCxxCnG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plot loss"
      ]
    },
    {
      "metadata": {
        "id": "OI1N73zkxEcC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "6f150dff-f361-4329-eddd-2a437208f9f7"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plot losses\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.plot(losses, linewidth = 1)\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Losses')\n",
        "    plt.ylim((0, 12))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAF3CAYAAACPPbA6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3ac49c7ef0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "kTbFJcy4xFsa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test our model!"
      ]
    },
    {
      "metadata": {
        "id": "kKtXLoZbxMYS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1314
        },
        "outputId": "570214db-9026-47e2-f74d-e84c4c15e655"
      },
      "cell_type": "code",
      "source": [
        "with tf.Graph().as_default():\n",
        "    \n",
        "    # placeholders\n",
        "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
        "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
        "\n",
        "    # output projection\n",
        "    size = 512\n",
        "    w_t = tf.get_variable('proj_w', [fr_vocab_size, size], tf.float32)\n",
        "    b = tf.get_variable('proj_b', [fr_vocab_size], tf.float32)\n",
        "    w = tf.transpose(w_t)\n",
        "    output_projection = (w, b)\n",
        "    \n",
        "    # change the model so that output at time t can be fed as input at time t+1\n",
        "    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
        "                                                encoder_inputs,\n",
        "                                                decoder_inputs,\n",
        "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
        "                                                num_encoder_symbols = en_vocab_size,\n",
        "                                                num_decoder_symbols = fr_vocab_size,\n",
        "                                                embedding_size = 100,\n",
        "                                                feed_previous = True, # <-----this is changed----->\n",
        "                                                output_projection = output_projection,\n",
        "                                                dtype = tf.float32)\n",
        "    \n",
        "    # ops for projecting outputs\n",
        "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
        "\n",
        "    # let's translate these sentences     \n",
        "    en_sentences = [\"What' s your name\", 'My name is', 'What are you doing', 'I am reading a book',\\\n",
        "                    'How are you', 'I am good', 'Do you speak English', 'What time is it', 'Hi', 'Goodbye', 'Yes', 'No']\n",
        "    en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
        "    \n",
        "    # padding to fit encoder input\n",
        "    for i in range(len(en_sentences_encoded)):\n",
        "        en_sentences_encoded[i] += (15 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
        "    \n",
        "    # restore all variables - use the last checkpoint saved\n",
        "    saver = tf.train.Saver()\n",
        "    path = tf.train.latest_checkpoint('checkpoints')\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        # restore\n",
        "        saver.restore(sess, path)\n",
        "        \n",
        "        # feed data into placeholders\n",
        "        feed = {}\n",
        "        for i in range(input_seq_len):\n",
        "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))], dtype = np.int32)\n",
        "            \n",
        "        feed[decoder_inputs[0].name] = np.array([fr_word2idx['<go>']] * len(en_sentences_encoded), dtype = np.int32)\n",
        "        \n",
        "        # translate\n",
        "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
        "        \n",
        "        # decode seq.\n",
        "        for i in range(len(en_sentences_encoded)):\n",
        "            print('{}.\\n--------------------------------'.format(i+1))\n",
        "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
        "            #decode output sequence\n",
        "            words = decode_output(ouput_seq)\n",
        "        \n",
        "            print(en_sentences[i])\n",
        "            \n",
        "            toPrt=\"\"\n",
        "            for i in range(len(words)):\n",
        "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
        "                    toPrt += words[i] + \" \"\n",
        "            print(toPrt)\n",
        "            \n",
        "            print('\\n--------------------------------')\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from checkpoints/-9999\n",
            "1.\n",
            "--------------------------------\n",
            "What' s your name\n",
            "Comment ça vous appelez- \n",
            "\n",
            "--------------------------------\n",
            "2.\n",
            "--------------------------------\n",
            "My name is\n",
            "Mon nom c'est \n",
            "\n",
            "--------------------------------\n",
            "3.\n",
            "--------------------------------\n",
            "What are you doing\n",
            "Qu'est- ce que tu fais \n",
            "\n",
            "--------------------------------\n",
            "4.\n",
            "--------------------------------\n",
            "I am reading a book\n",
            "Je suis le livre \n",
            "\n",
            "--------------------------------\n",
            "5.\n",
            "--------------------------------\n",
            "How are you\n",
            "- Comment ça \n",
            "\n",
            "--------------------------------\n",
            "6.\n",
            "--------------------------------\n",
            "I am good\n",
            "Je suis top \n",
            "\n",
            "--------------------------------\n",
            "7.\n",
            "--------------------------------\n",
            "Do you speak English\n",
            "Tu te parle \n",
            "\n",
            "--------------------------------\n",
            "8.\n",
            "--------------------------------\n",
            "What time is it\n",
            "Qu'est- ce que c'est quoi \n",
            "\n",
            "--------------------------------\n",
            "9.\n",
            "--------------------------------\n",
            "Hi\n",
            "Bonsoir \n",
            "\n",
            "--------------------------------\n",
            "10.\n",
            "--------------------------------\n",
            "Goodbye\n",
            "Adieu \n",
            "\n",
            "--------------------------------\n",
            "11.\n",
            "--------------------------------\n",
            "Yes\n",
            "Oui \n",
            "\n",
            "--------------------------------\n",
            "12.\n",
            "--------------------------------\n",
            "No\n",
            "Non \n",
            "\n",
            "--------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C_g7Mm11xLXr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}